{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_nGIKu-Jz6C"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üî¥ PART 1: CORE DESIGN"
      ],
      "metadata": {
        "id": "yROgwHU-Lymb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **1. End-to-End LLM Feature Design**\n",
        "\n",
        "**Quick Framework (use for ANY design question):**\n",
        "\n",
        "```\n",
        "1. Clarify ‚Üí 2. Simple baseline ‚Üí 3. Where it breaks ‚Üí 4. How to improve\n",
        "```\n",
        "\n",
        "**Example: Customer Support Chatbot**\n",
        "\n",
        "**Clarify:**\n",
        "- Volume? (100 tickets/day vs 10K/day)\n",
        "- Latency requirement? (<2s for chat)\n",
        "- Knowledge base? (docs, past tickets, FAQs)\n",
        "- Budget constraints?\n",
        "\n",
        "**Baseline Design:**\n",
        "```\n",
        "User Query ‚Üí Embedding ‚Üí Vector Search (top-3 chunks)\n",
        "‚Üí LLM (GPT-4/Claude) with context ‚Üí Response\n",
        "```\n",
        "\n",
        "**Why RAG vs Fine-tuning?**\n",
        "- **RAG when:** Knowledge changes frequently, need citations, limited budget\n",
        "- **Fine-tuning when:** Specific tone/style needed, repeated patterns, cost matters long-term\n",
        "\n",
        "**Caching strategy:**\n",
        "- Cache embeddings (they don't change)\n",
        "- Cache common queries at response level\n",
        "- Don't cache final LLM calls (context varies)\n",
        "\n",
        "**Evaluation:**\n",
        "- Offline: retrieval precision@k, answer relevance (LLM-as-judge)\n",
        "- Online: thumbs up/down, resolution rate, human spot-checks\n",
        "\n",
        "**Cost control:**\n",
        "- Smaller model for routing/classification\n",
        "- Larger model only for complex queries\n",
        "- Prompt compression, context pruning\n",
        "\n",
        "---\n",
        "\n",
        "### **2. RAG-Specific Debugging**\n",
        "\n",
        "#### **\"Answers are confident but wrong\"**\n",
        "\n",
        "**Debug checklist:**\n",
        "1. **Check retrieval first** ‚Üí Are right docs retrieved? (log top-3 chunks)\n",
        "2. **Context position** ‚Üí Move most relevant chunk to end (recency bias)\n",
        "3. **Chunk quality** ‚Üí Too small = missing context, too large = noise\n",
        "4. **Prompt instruction** ‚Üí Add \"Only use provided context, admit if unsure\"\n",
        "\n",
        "#### **\"Retrieval correct, generation bad\"**\n",
        "\n",
        "- **Model ignoring context** ‚Üí Use stronger instruction, add examples (few-shot)\n",
        "- **Context too long** ‚Üí Model lost in middle, use re-ranking or summarization first\n",
        "- **Conflicting info in context** ‚Üí Pre-filter contradictory chunks\n",
        "\n",
        "#### **\"Larger chunks help recall but hurt quality\"**\n",
        "\n",
        "**Solution hierarchy:**\n",
        "1. **Hybrid chunking** ‚Üí Keep small chunks, but retrieve parent chunks for context\n",
        "2. **Two-stage retrieval** ‚Üí Small chunks for search, expand window for generation\n",
        "3. **Sliding window overlap** ‚Üí 20-30% overlap between chunks\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Model Choice & Adaptation**\n",
        "\n",
        "#### **Open-source vs API**\n",
        "\n",
        "| Factor | Open-source | API |\n",
        "|--------|-------------|-----|\n",
        "| Cost | High upfront, cheap at scale | Pay-per-token |\n",
        "| Control | Full (prompts, weights) | Limited |\n",
        "| Latency | You control infra | Network dependent |\n",
        "| Compliance | Keep data internal | Data leaves premises |\n",
        "\n",
        "**Startup default:** Start with API (GPT-4/Claude), switch to open-source when cost > $5K/month\n",
        "\n",
        "#### **LoRA vs Full Fine-tuning**\n",
        "\n",
        "**Use LoRA when:**\n",
        "- Limited data (<10K examples)\n",
        "- Need multiple task-specific models\n",
        "- Fast iteration needed\n",
        "\n",
        "**Use Full FT when:**\n",
        "- Domain shift is massive (legal ‚Üí medical)\n",
        "- Have 50K+ high-quality examples\n",
        "- Model architecture change needed\n",
        "\n",
        "**Avoid PEFT when:**\n",
        "- Base model is already very small (<3B params)\n",
        "- Task needs architectural changes (new tokens, special layers)\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Evaluation & Metrics**\n",
        "\n",
        "#### **\"Loss decreasing but quality worse\"**\n",
        "\n",
        "**Common causes:**\n",
        "1. **Train-test mismatch** ‚Üí Training on clean data, users give messy input\n",
        "2. **Overfitting to metric** ‚Üí Model gaming the loss function\n",
        "3. **Proxy metric misalignment** ‚Üí Optimizing BLEU but users want factuality\n",
        "\n",
        "**Fix:** Add hold-out set from real production data, use human eval\n",
        "\n",
        "#### **\"How to evaluate GenAI before launch?\"**\n",
        "\n",
        "**3-tier system:**\n",
        "\n",
        "1. **Automated (fast, cheap)**\n",
        "   - Retrieval: precision@3, MRR\n",
        "   - Generation: perplexity, length, refusal rate\n",
        "   - LLM-as-judge for relevance (GPT-4 rating 1-5)\n",
        "\n",
        "2. **Human spot-check (medium)**\n",
        "   - 100-200 examples weekly\n",
        "   - Domain expert review for factuality\n",
        "\n",
        "3. **Production metrics (slow, real)**\n",
        "   - User engagement (thumbs, session length)\n",
        "   - Task success (ticket resolved? query answered?)\n",
        "\n",
        "**Preventing regression:**\n",
        "- Golden test set (100 hand-labeled examples) ‚Üí run on every model update\n",
        "- Shadow deployment ‚Üí new model runs in parallel, compare outputs\n",
        "- Gradual rollout (5% ‚Üí 20% ‚Üí 100%)\n",
        "\n",
        "---\n",
        "\n",
        "### **Quick Recall Bullets for Part 1:**\n",
        "\n",
        "‚úÖ **RAG vs Fine-tuning:** RAG for changing knowledge + citations, FT for style + cost at scale  \n",
        "‚úÖ **RAG debug:** Check retrieval ‚Üí chunk quality ‚Üí prompt strength ‚Üí context position  \n",
        "‚úÖ **Model choice:** API first, open-source at $5K+/month  \n",
        "‚úÖ **LoRA:** Use for <10K data, multi-task, fast iteration  \n",
        "‚úÖ **Eval:** Automated (LLM judge) + Human spot-check + Production metrics  \n",
        "‚úÖ **Loss ‚â† Quality:** Always validate on real production distribution  \n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "pb_caPewLp4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 2\n",
        "\n",
        "üü† PART 2: APPLIED SYSTEM THINKING\n",
        "üü° PART 2: OPTIMIZATION & SCALING"
      ],
      "metadata": {
        "id": "ts3Eha7lTzEg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üü† PART 2: APPLIED SYSTEM THINKING"
      ],
      "metadata": {
        "id": "V3R4VGuoMR0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **5. Performance & Cost Tradeoffs**\n",
        "\n",
        "#### **\"Latency doubled after launch\"**\n",
        "\n",
        "**Systematic debug approach:**\n",
        "\n",
        "1. **Add logging at each stage:**\n",
        "   ```\n",
        "   Retrieval: 200ms\n",
        "   Embedding: 50ms\n",
        "   LLM call: 2500ms ‚Üê culprit\n",
        "   Post-processing: 100ms\n",
        "   ```\n",
        "\n",
        "2. **Common causes by component:**\n",
        "   - **Retrieval slow:** Vector DB overloaded, missing index, cold start\n",
        "   - **LLM slow:** Context too long, batch size = 1, no KV cache\n",
        "   - **Network:** API rate limits, timeout retries adding up\n",
        "\n",
        "3. **Quick wins:**\n",
        "   - Enable streaming (perceived latency drops)\n",
        "   - Cache embeddings\n",
        "   - Reduce max_tokens if over-generating\n",
        "   - Parallel retrieval + embedding if possible\n",
        "\n",
        "#### **\"Tokens per request keep increasing\"**\n",
        "\n",
        "**Root causes:**\n",
        "- Conversation history grows unbounded\n",
        "- Users copy-pasting large documents\n",
        "- System prompts bloating over time\n",
        "\n",
        "**Fixes:**\n",
        "1. **Sliding window:** Keep only last 5 messages in context\n",
        "2. **Summarization:** Compress old history (every 10 turns ‚Üí summarize)\n",
        "3. **Prompt pruning:** Remove redundant instructions\n",
        "4. **Hard limits:** Cap user input at 2K tokens, context at 8K tokens\n",
        "\n",
        "#### **\"Reduce inference cost without retraining\"**\n",
        "\n",
        "**Immediate actions (sorted by impact):**\n",
        "\n",
        "1. **Switch to cheaper model for simple queries** (70% of queries don't need GPT-4)\n",
        "   - Use classifier: GPT-3.5 for routing ‚Üí GPT-4 only if needed\n",
        "2. **Prompt compression** (LLMLingua, remove filler words)\n",
        "3. **Quantization** (16bit ‚Üí 8bit, 50% cost cut, minimal quality loss)\n",
        "4. **Reduce temperature** (less sampling = faster, cheaper)\n",
        "5. **Cache aggressively** (embed common questions, cache similar queries)\n",
        "\n",
        "#### **\"Batch vs Streaming tradeoffs\"**\n",
        "\n",
        "| Aspect | Batch | Streaming |\n",
        "|--------|-------|-----------|\n",
        "| **Latency** | High (wait for full response) | Low (perceived, shows tokens immediately) |\n",
        "| **Throughput** | Higher (GPU efficient) | Lower (harder to batch) |\n",
        "| **Cost** | Cheaper (better GPU util) | More expensive |\n",
        "| **UX** | Feels slow | Feels fast |\n",
        "| **Use case** | Background jobs, bulk processing | Chat, interactive |\n",
        "\n",
        "**Startup rule:** Streaming for user-facing chat, batch for backend processing\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Failure Modes & Debugging**\n",
        "\n",
        "#### **\"Model hallucinates despite correct context\"**\n",
        "\n",
        "**Why it happens:**\n",
        "- Model's parametric memory conflicts with context\n",
        "- Context buried in middle of long prompt\n",
        "- Weak instruction adherence\n",
        "\n",
        "**Fixes (in order of effort):**\n",
        "1. **Stronger system prompt:**\n",
        "   ```\n",
        "   \"You MUST only use information from the context below.\n",
        "   If the answer isn't in the context, say 'I don't have that information.'\"\n",
        "   ```\n",
        "2. **Few-shot examples** showing correct refusals\n",
        "3. **Post-processing filter** (another LLM checks if answer uses context)\n",
        "4. **Fine-tune** on examples with correct refusal behavior\n",
        "\n",
        "#### **\"Model ignores system prompt occasionally\"**\n",
        "\n",
        "**Common pattern:** Works 90% of time, fails randomly\n",
        "\n",
        "**Causes:**\n",
        "- System prompt too long (model loses focus)\n",
        "- Conflicting instructions (system says X, user says Y)\n",
        "- Model trained to be helpful > follow rules\n",
        "\n",
        "**Fixes:**\n",
        "1. **Shorten system prompt** (80% of instructions are redundant)\n",
        "2. **Repeat critical rules** at start AND end of prompt\n",
        "3. **Use delimiters:**\n",
        "   ```\n",
        "   ### CRITICAL RULE ###\n",
        "   Never make up information\n",
        "   ### END CRITICAL RULE ###\n",
        "   ```\n",
        "4. **Switch model** (Claude > GPT for instruction-following)\n",
        "\n",
        "#### **\"Answers degrade for long conversations\"**\n",
        "\n",
        "**Why:**\n",
        "- Context window fills up ‚Üí early messages truncated\n",
        "- Attention dilutes over long sequences\n",
        "- Contradictory info accumulates\n",
        "\n",
        "**Fixes:**\n",
        "1. **Summarize every N turns** (keep intent, drop verbosity)\n",
        "2. **Prune low-importance turns** (greetings, confirmations)\n",
        "3. **Reset with context transfer:**\n",
        "   ```\n",
        "   After 10 turns: \"Here's what we discussed: [summary]. Continue from here.\"\n",
        "   ```\n",
        "\n",
        "#### **\"Works in tests, fails in production\"**\n",
        "\n",
        "**The distribution shift problem:**\n",
        "\n",
        "**Common mismatches:**\n",
        "- **Test:** Clean, well-formed queries\n",
        "- **Prod:** Typos, slang, vague questions, multi-intent\n",
        "\n",
        "**Debug process:**\n",
        "1. **Log 100 production failures** ‚Üí manually categorize\n",
        "2. **Build adversarial test set** from real failures\n",
        "3. **Add data augmentation** (typos, paraphrases) to eval\n",
        "\n",
        "**Prevention:**\n",
        "- Always test on real user data before launch\n",
        "- Shadow mode for 1 week minimum\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Prompting vs Training**\n",
        "\n",
        "#### **\"When is prompting enough?\"**\n",
        "\n",
        "**Prompting works when:**\n",
        "- Task is clear and well-defined\n",
        "- Model already knows the domain\n",
        "- You need fast iteration\n",
        "- < 100 examples available\n",
        "- Cost isn't the bottleneck\n",
        "\n",
        "**Examples:** Summarization, Q&A on public knowledge, code generation\n",
        "\n",
        "#### **\"When does prompt engineering break down?\"**\n",
        "\n",
        "**Red flags prompting won't scale:**\n",
        "1. **Prompt > 3000 tokens** (context stuffing)\n",
        "2. **Need 10+ few-shot examples** (just fine-tune)\n",
        "3. **Desired behavior contradicts base model** (e.g., make Llama give very terse answers)\n",
        "4. **Cost > $1K/month** on prompts alone\n",
        "\n",
        "**Migration trigger:** If spending 5+ hours/week tweaking prompts ‚Üí fine-tune\n",
        "\n",
        "#### **\"How to version and test prompts?\"**\n",
        "\n",
        "**Simple system (enough for startups):**\n",
        "\n",
        "```python\n",
        "prompts/\n",
        "  v1_baseline.txt\n",
        "  v2_add_examples.txt (A/B test vs v1)\n",
        "  v3_shorter.txt\n",
        "\n",
        "# Git commit message:\n",
        "\"Prompt v2: Added 3 examples for edge cases, reduced hallucination by 15%\"\n",
        "```\n",
        "\n",
        "**Testing:**\n",
        "1. **Golden set:** 50-100 hand-labeled examples\n",
        "2. **Automated eval:** LLM-as-judge scores both versions\n",
        "3. **A/B test:** 10% traffic for 24 hours\n",
        "4. **Decision rule:** Win on both automated + human eval\n",
        "\n",
        "#### **\"Migrate from prompts ‚Üí fine-tuning\"**\n",
        "\n",
        "**When to switch:**\n",
        "- Prompt cost > $500/month\n",
        "- Latency matters (shorter prompts = faster)\n",
        "- Need consistent style/format\n",
        "- Have 500+ good examples\n",
        "\n",
        "**How to prepare:**\n",
        "1. **Log best prompt outputs** ‚Üí becomes training data\n",
        "2. **Extract system prompt patterns** ‚Üí bake into fine-tuning\n",
        "3. **Keep simple prompt post-FT** (you still need some instruction)\n",
        "\n",
        "**Caution:** Don't fine-tune too early, prompts are more flexible\n",
        "\n",
        "---\n",
        "\n",
        "### **8. Data & Drift**\n",
        "\n",
        "#### **\"Data distribution changes silently ‚Äî how to detect?\"**\n",
        "\n",
        "**Monitoring system (minimum viable):**\n",
        "\n",
        "1. **Input drift:**\n",
        "   - Track input length distribution (weekly)\n",
        "   - Monitor top-K query patterns (clustering)\n",
        "   - Alert if new query types > 20% of traffic\n",
        "\n",
        "2. **Output drift:**\n",
        "   - Track average output length\n",
        "   - Monitor refusal rate (sudden spike = problem)\n",
        "   - LLM-as-judge quality score (weekly batch)\n",
        "\n",
        "3. **User feedback:**\n",
        "   - Thumbs down rate\n",
        "   - Session abandonment\n",
        "\n",
        "**Alert triggers:**\n",
        "- Refusal rate changes > 10%\n",
        "- Average quality score drops > 0.3\n",
        "- Thumbs down increases > 15%\n",
        "\n",
        "#### **\"Retrieval drift vs Model drift\"**\n",
        "\n",
        "| Type | What changed | How to detect | Fix |\n",
        "|------|--------------|---------------|-----|\n",
        "| **Retrieval drift** | Knowledge base updated, embeddings stale | Retrieval quality drops, same model | Re-embed documents |\n",
        "| **Model drift** | User language shifts, model outdated | Retrieval fine, outputs feel off | Fine-tune or update base model |\n",
        "\n",
        "**Real example:**\n",
        "- Company rebrands product name\n",
        "- Old docs still use old name\n",
        "- Retrieval fails ‚Üí **Retrieval drift** (update docs)\n",
        "\n",
        "#### **\"User behavior shifts ‚Äî model correct but useless\"**\n",
        "\n",
        "**Scenario:** Users start asking advanced questions, but model gives beginner-level answers\n",
        "\n",
        "**Why metrics don't catch it:**\n",
        "- Answers are still factually correct\n",
        "- Automated eval passes\n",
        "- But users are frustrated (not what they need)\n",
        "\n",
        "**Detection:**\n",
        "- **Qualitative feedback:** \"Too basic\", \"I know this already\"\n",
        "- **Engagement drops:** Session length decreases\n",
        "- **Query reformulation:** Users keep asking follow-ups\n",
        "\n",
        "**Fix:**\n",
        "- Segment users (beginner vs advanced)\n",
        "- Adapt system prompt based on user segment\n",
        "- Add complexity level to prompt: \"Assume expert-level knowledge\"\n",
        "\n",
        "---\n",
        "\n",
        "### **Quick Recall Bullets for Part 2:**\n",
        "\n",
        "‚úÖ **Latency debug:** Log each component ‚Üí find bottleneck ‚Üí cache/parallelize/optimize  \n",
        "‚úÖ **Token cost:** Sliding window, summarize history, hard caps, route to cheaper models  \n",
        "‚úÖ **Hallucination:** Stronger instruction + few-shot + context position  \n",
        "‚úÖ **System prompt ignored:** Too long, conflicting rules ‚Üí shorten + repeat critical parts  \n",
        "‚úÖ **Prompt ‚Üí FT:** Switch when cost > $500/mo OR latency critical OR have 500+ examples  \n",
        "‚úÖ **Drift detection:** Monitor input/output distributions, refusal rate, quality scores  \n",
        "‚úÖ **Test ‚â† Prod:** Always log real failures, build adversarial test set from production  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "bBiYatO3MTj7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üü° PART 2: OPTIMIZATION & SCALING"
      ],
      "metadata": {
        "id": "HutY2xKhTauU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **9. Inference Optimization Scenarios**\n",
        "\n",
        "#### **\"Why does latency grow with conversation length?\"**\n",
        "\n",
        "**Root cause: Context window processing**\n",
        "\n",
        "Every turn, the model reprocesses the entire conversation:\n",
        "```\n",
        "Turn 1: Process 100 tokens ‚Üí 200ms\n",
        "Turn 5: Process 500 tokens ‚Üí 1000ms\n",
        "Turn 10: Process 1000 tokens ‚Üí 2000ms\n",
        "```\n",
        "\n",
        "**Why it's quadratic-ish:**\n",
        "- Self-attention is O(n¬≤) in sequence length\n",
        "- Each new token attends to ALL previous tokens\n",
        "- No memory of previous computations (naive implementation)\n",
        "\n",
        "**Solutions:**\n",
        "\n",
        "1. **KV-cache (most important):**\n",
        "   - Cache key/value matrices from previous turns\n",
        "   - Only compute attention for new tokens\n",
        "   - Reduces Turn 10 from 2000ms ‚Üí 250ms\n",
        "   - **Tradeoff:** Uses more memory (store KV for all tokens)\n",
        "\n",
        "2. **Conversation compression:**\n",
        "   - Summarize every 5 turns\n",
        "   - Keep last 3 messages verbatim, summarize older ones\n",
        "\n",
        "3. **Stateful sessions:**\n",
        "   - Store conversation state on server\n",
        "   - Don't re-send full history every time\n",
        "\n",
        "#### **\"How does KV-cache help and when does it fail?\"**\n",
        "\n",
        "**How it works:**\n",
        "```\n",
        "Without KV-cache:\n",
        "Turn 1: Compute attention for tokens [1-100]\n",
        "Turn 2: Recompute attention for tokens [1-200] ‚Üê wasteful\n",
        "\n",
        "With KV-cache:\n",
        "Turn 1: Compute + cache K,V for tokens [1-100]\n",
        "Turn 2: Reuse cached [1-100], compute only [101-200] ‚Üê 2x faster\n",
        "```\n",
        "\n",
        "**When KV-cache fails:**\n",
        "\n",
        "1. **Memory overflow:**\n",
        "   - Long conversations fill GPU memory\n",
        "   - Cache size grows linearly with context\n",
        "   - **Fix:** Evict old KV pairs (sliding window)\n",
        "\n",
        "2. **Batch processing:**\n",
        "   - Different conversations have different lengths\n",
        "   - Can't batch efficiently (padding wastes compute)\n",
        "   - **Fix:** PagedAttention (vLLM), dynamic batching\n",
        "\n",
        "3. **Context modification:**\n",
        "   - If you edit earlier messages, cache is invalid\n",
        "   - Must recompute from scratch\n",
        "\n",
        "#### **\"vLLM vs HuggingFace generate ‚Äî when to choose?\"**\n",
        "\n",
        "| Feature | HuggingFace `.generate()` | vLLM |\n",
        "|---------|---------------------------|------|\n",
        "| **Ease of use** | Very simple, 3 lines of code | Needs server setup |\n",
        "| **Throughput** | Low (naive batching) | High (PagedAttention, continuous batching) |\n",
        "| **Latency (single)** | Same | Same |\n",
        "| **Latency (concurrent)** | Poor (queue waits) | Good (batches efficiently) |\n",
        "| **Memory efficiency** | Moderate | High (dynamic KV allocation) |\n",
        "| **Use case** | Prototyping, low traffic | Production, high traffic |\n",
        "\n",
        "**Startup decision rule:**\n",
        "- **<10 req/min:** HuggingFace is fine\n",
        "- **>50 req/min:** Use vLLM or similar (TGI, TensorRT-LLM)\n",
        "\n",
        "**Why vLLM wins at scale:**\n",
        "- **PagedAttention:** KV-cache stored in non-contiguous memory (like OS virtual memory)\n",
        "- **Continuous batching:** New requests join ongoing batches without waiting\n",
        "- Result: 2-3x higher throughput, 50% lower latency under load\n",
        "\n",
        "#### **\"How do you handle bursty traffic?\"**\n",
        "\n",
        "**Problem:**\n",
        "- Normal: 10 req/min\n",
        "- Peak: 200 req/min (product launch, viral moment)\n",
        "- Infrastructure sized for normal ‚Üí peak = downtime\n",
        "\n",
        "**Solutions (by sophistication):**\n",
        "\n",
        "1. **Quick fix: Queue + backpressure**\n",
        "   ```\n",
        "   if queue_length > 50:\n",
        "       return \"High traffic, please retry in 30s\"\n",
        "   ```\n",
        "   - Ugly but works for rare spikes\n",
        "\n",
        "2. **Auto-scaling (cloud):**\n",
        "   - Spin up more GPU instances on demand\n",
        "   - **Issue:** Cold start = 2-5 minutes (can't help sudden spikes)\n",
        "   - **Hybrid:** Keep 1-2 warm standby instances\n",
        "\n",
        "3. **Request prioritization:**\n",
        "   - Paying users ‚Üí high priority queue\n",
        "   - Free tier ‚Üí low priority, shed load if needed\n",
        "\n",
        "4. **Model cascade:**\n",
        "   ```\n",
        "   High traffic ‚Üí route simple queries to smaller/faster model\n",
        "   Complex queries ‚Üí still use best model\n",
        "   ```\n",
        "\n",
        "5. **Rate limiting (preventive):**\n",
        "   - Per-user limits (10 req/min)\n",
        "   - Exponential backoff for repeated requests\n",
        "\n",
        "**Startup MVP:** Queue + simple rate limiting, monitor 95th percentile latency\n",
        "\n",
        "---\n",
        "\n",
        "### **10. Quantization & Deployment**\n",
        "\n",
        "#### **\"Quantized model ‚Üí accuracy dropped. Why?\"**\n",
        "\n",
        "**What quantization does:**\n",
        "```\n",
        "FP16: 16 bits per parameter\n",
        "INT8: 8 bits per parameter ‚Üí 2x smaller, 2x faster\n",
        "INT4: 4 bits ‚Üí 4x smaller, but risky\n",
        "```\n",
        "\n",
        "**Common failure modes:**\n",
        "\n",
        "1. **Outlier features:**\n",
        "   - Some activations are 100x larger than others\n",
        "   - Quantizing them loses critical info\n",
        "   - **Fix:** Mixed precision (keep outliers in FP16)\n",
        "\n",
        "2. **Calibration data mismatch:**\n",
        "   - Quantization uses sample data to set scale factors\n",
        "   - If calibration data ‚â† production data ‚Üí poor performance\n",
        "   - **Fix:** Calibrate on real production samples\n",
        "\n",
        "3. **Too aggressive (INT4 on small models):**\n",
        "   - < 7B params models are sensitive to INT4\n",
        "   - **Rule:** FP16 for < 3B, INT8 for 3-13B, INT4 only for 70B+\n",
        "\n",
        "4. **Wrong quantization method:**\n",
        "   - Per-tensor quantization (simple, lossy)\n",
        "   - Per-channel quantization (better, standard)\n",
        "   - Group-wise quantization (best, GPTQ/AWQ)\n",
        "\n",
        "#### **\"GPTQ vs AWQ ‚Äî what to choose?\"**\n",
        "\n",
        "| Method | GPTQ | AWQ |\n",
        "|--------|------|-----|\n",
        "| **Calibration time** | Slow (hours for 70B) | Fast (minutes) |\n",
        "| **Quality (INT4)** | Good | Better |\n",
        "| **How it works** | Minimize quantization error globally | Protect important weights (activation-aware) |\n",
        "| **Best for** | General use, batch processing | Low-latency serving |\n",
        "\n",
        "**Startup rule:**\n",
        "- Default: **AWQ** (faster calibration, slightly better quality)\n",
        "- Use GPTQ if: Already have infrastructure for it\n",
        "\n",
        "**Both require:**\n",
        "- GPU for inference (quantized models still need GPU)\n",
        "- Proper kernel support (AutoGPTQ, AutoAWQ libraries)\n",
        "\n",
        "#### **\"How to A/B test quantized models safely?\"**\n",
        "\n",
        "**Challenge:** Quality degradation is subtle, not binary\n",
        "\n",
        "**Safe rollout process:**\n",
        "\n",
        "1. **Shadow mode (1 week):**\n",
        "   - Run quantized model in parallel\n",
        "   - Log outputs, don't serve to users\n",
        "   - Compare: exact match rate, LLM-as-judge similarity score\n",
        "\n",
        "2. **Canary (5% traffic):**\n",
        "   - Serve to 5% of users\n",
        "   - Monitor: thumbs down rate, session length, refusal rate\n",
        "   - **Kill switch:** Auto-rollback if metrics degrade >10%\n",
        "\n",
        "3. **Gradual ramp (5% ‚Üí 20% ‚Üí 50% ‚Üí 100%):**\n",
        "   - Each stage lasts 2-3 days\n",
        "   - Pause if any metric regresses\n",
        "\n",
        "**Metrics to watch:**\n",
        "- Task success rate (primary)\n",
        "- User satisfaction (thumbs up/down)\n",
        "- Refusal rate (quantization can make model refuse more)\n",
        "- Output length (sometimes drops, bad sign)\n",
        "\n",
        "**Red flags ‚Üí immediate rollback:**\n",
        "- Refusal rate increases >20%\n",
        "- Thumbs down increases >15%\n",
        "- Silent failures (empty outputs, broken JSON)\n",
        "\n",
        "---\n",
        "\n",
        "### **11. Model Updates & Versioning**\n",
        "\n",
        "#### **\"How do you roll out a new model version?\"**\n",
        "\n",
        "**Standard process (low-risk):**\n",
        "\n",
        "1. **Offline validation:**\n",
        "   - Golden test set (100 examples) ‚Üí must match or beat old model\n",
        "   - Edge case set (adversarial examples) ‚Üí no new failures\n",
        "   - Cost/latency benchmark\n",
        "\n",
        "2. **Shadow deployment (3-7 days):**\n",
        "   ```\n",
        "   User query ‚Üí Old model (serve this)\n",
        "                ‚Üì\n",
        "              New model (log only, compare)\n",
        "   ```\n",
        "   - Compare outputs: agreement rate, quality scores\n",
        "   - Look for: new failure modes, unexpected behaviors\n",
        "\n",
        "3. **Canary (5% for 2-3 days):**\n",
        "   - Serve new model to 5% of users\n",
        "   - Monitor real-time metrics\n",
        "   - **Decision:** If metrics stable or better ‚Üí continue\n",
        "   - **Rollback trigger:** Any metric regresses >10%\n",
        "\n",
        "4. **Gradual rollout:**\n",
        "   - 5% ‚Üí 20% ‚Üí 50% ‚Üí 100%\n",
        "   - Each stage: wait 48 hours, check metrics\n",
        "   - Keep old model warm (can rollback instantly)\n",
        "\n",
        "5. **Full deployment:**\n",
        "   - Deprecate old model after 1 week of stability\n",
        "   - Keep old model archived for 1 month (just in case)\n",
        "\n",
        "#### **\"How do you rollback quickly?\"**\n",
        "\n",
        "**Pre-rollout setup:**\n",
        "\n",
        "```python\n",
        "# Feature flag system\n",
        "if feature_flag.get(\"model_version\") == \"v2\":\n",
        "    model = load_model_v2()\n",
        "else:\n",
        "    model = load_model_v1()  # fallback\n",
        "```\n",
        "\n",
        "**Instant rollback (< 1 minute):**\n",
        "- Flip feature flag\n",
        "- No redeployment needed\n",
        "- Old model still running in background\n",
        "\n",
        "**Infrastructure requirements:**\n",
        "- Keep old model loaded in memory (costs extra RAM)\n",
        "- OR: Accept 30s cold-start on rollback\n",
        "- Load balancer can switch traffic instantly\n",
        "\n",
        "**When to rollback:**\n",
        "- Error rate spike\n",
        "- User complaints surge\n",
        "- Metrics drop significantly\n",
        "- Silent failures detected\n",
        "\n",
        "**Startup mistake to avoid:**\n",
        "- Don't shut down old model immediately after new model launches\n",
        "- Keep both running for 48 hours minimum\n",
        "\n",
        "#### **\"How do you compare two LLMs fairly?\"**\n",
        "\n",
        "**The challenge:** No single \"correct\" answer\n",
        "\n",
        "**Multi-dimensional evaluation:**\n",
        "\n",
        "1. **Task success (objective):**\n",
        "   - Retrieval: Did it find right document?\n",
        "   - Extraction: Did it extract correct entities?\n",
        "   - Classification: Did it classify correctly?\n",
        "\n",
        "2. **Quality (subjective but measurable):**\n",
        "   - **LLM-as-judge (GPT-4):**\n",
        "     ```\n",
        "     \"Compare these two answers. Rate 1-5 on:\n",
        "     - Accuracy\n",
        "     - Helpfulness\n",
        "     - Conciseness\"\n",
        "     ```\n",
        "   - Run on 200 diverse examples\n",
        "   - Calculate win rate (Model A better, B better, tie)\n",
        "\n",
        "3. **Pairwise human eval:**\n",
        "   - Show annotators both outputs (blind labels)\n",
        "   - \"Which answer is better?\" ‚Üí A, B, or Tie\n",
        "   - Need ~100 comparisons for statistical significance\n",
        "   - **Inter-annotator agreement** must be >70%\n",
        "\n",
        "4. **Production metrics (most important):**\n",
        "   - User engagement (thumbs, session length)\n",
        "   - Task completion rate\n",
        "   - Cost per task\n",
        "   - Latency\n",
        "\n",
        "**Fair comparison checklist:**\n",
        "- ‚úÖ Same prompts, same retrieval, same system message\n",
        "- ‚úÖ Same temperature and generation config\n",
        "- ‚úÖ Test on representative sample (not cherry-picked)\n",
        "- ‚úÖ Measure multiple metrics (not just one)\n",
        "- ‚úÖ Statistical significance test (bootstrap, t-test)\n",
        "\n",
        "**Reporting:**\n",
        "```\n",
        "Model A vs Model B:\n",
        "- Task success: 85% vs 87% (+2%, p=0.04) ‚úì significant\n",
        "- LLM-judge: 4.2 vs 4.3 (+0.1, p=0.23) ‚úó not significant\n",
        "- Latency: 1.2s vs 1.8s (+50%) ‚úó worse\n",
        "- Cost: $0.02 vs $0.015 (-25%) ‚úì cheaper\n",
        "\n",
        "Decision: Keep Model A (faster, slightly worse quality not worth cost)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Quick Recall Bullets for Part 3:**\n",
        "\n",
        "‚úÖ **Latency + context length:** KV-cache is critical, reduces recomputation  \n",
        "‚úÖ **vLLM vs HF:** vLLM for >50 req/min, HF for prototyping  \n",
        "‚úÖ **Bursty traffic:** Queue + rate limiting + auto-scale, monitor P95 latency  \n",
        "‚úÖ **Quantization:** INT8 safe for 7B+, INT4 only for 70B+, calibrate on prod data  \n",
        "‚úÖ **GPTQ vs AWQ:** AWQ faster + slightly better, default choice  \n",
        "‚úÖ **Model rollout:** Shadow ‚Üí Canary (5%) ‚Üí Gradual (20/50/100) ‚Üí Keep old model warm  \n",
        "‚úÖ **Rollback:** Feature flags, keep old model running 48h, instant switch  \n",
        "‚úÖ **LLM comparison:** LLM-as-judge + human pairwise + production metrics, need statistical significance  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "QMdb2uC3MvFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üü¢ P3 ADVANCED / DIFFERENTIATOR"
      ],
      "metadata": {
        "id": "telbKzE3Mw-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **12. Research-Flavored Scenarios**\n",
        "\n",
        "#### **\"How would you test a new attention mechanism?\"**\n",
        "\n",
        "**Context:** Paper claims \"Sparse Attention improves efficiency with no quality loss\"\n",
        "\n",
        "**Validation process (startup-practical):**\n",
        "\n",
        "1. **Reproducibility check (1-2 days):**\n",
        "   - Can you actually run their code?\n",
        "   - Do their numbers match on standard benchmarks?\n",
        "   - **Red flag:** If you can't reproduce ‚Üí skip it\n",
        "\n",
        "2. **Your domain test (3-5 days):**\n",
        "   ```\n",
        "   Test on YOUR actual use case, not paper's benchmarks\n",
        "   \n",
        "   Example: Customer support chatbot\n",
        "   - Run 500 real production queries\n",
        "   - Measure: quality (LLM-judge), latency, memory\n",
        "   ```\n",
        "\n",
        "3. **Edge cases (2 days):**\n",
        "   - Very long contexts (paper tested 4K, you need 16K)\n",
        "   - Diverse query types (paper used Q&A, you have multi-turn chat)\n",
        "   - Failure modes (does it break unexpectedly?)\n",
        "\n",
        "4. **Engineering cost (critical):**\n",
        "   - Implementation time: 1 day vs 2 weeks?\n",
        "   - Maintenance burden: custom CUDA kernels vs drop-in replacement?\n",
        "   - Team expertise: Do you have people who can debug this?\n",
        "\n",
        "**Decision framework:**\n",
        "```\n",
        "Ship if:\n",
        "‚úì 10%+ quality improvement OR 30%+ speedup\n",
        "‚úì Works on your data (not just paper's)\n",
        "‚úì Implementation cost < 1 week\n",
        "‚úì No new critical failure modes\n",
        "\n",
        "Skip if:\n",
        "‚úó Marginal gains (<5% on your data)\n",
        "‚úó Requires custom infrastructure\n",
        "‚úó Team can't maintain it\n",
        "```\n",
        "\n",
        "**Startup reality:** Most research optimizations aren't worth it unless gains are huge (>20%)\n",
        "\n",
        "#### **\"How do you decide if a paper is production-worthy?\"**\n",
        "\n",
        "**Quick filter (read in 15 mins):**\n",
        "\n",
        "1. **Code available?**\n",
        "   - No code = not production-ready\n",
        "   - Code but no pretrained weights = rebuild cost too high\n",
        "\n",
        "2. **Realistic evaluation?**\n",
        "   - Paper tested on academic benchmarks only ‚Üí skeptical\n",
        "   - Paper includes production scenarios, latency, cost ‚Üí promising\n",
        "\n",
        "3. **Comparison to strong baselines?**\n",
        "   - \"Beats GPT-2\" (2019 model) ‚Üí not useful\n",
        "   - \"Beats GPT-4 on X\" ‚Üí interesting\n",
        "\n",
        "4. **Clear failure modes discussed?**\n",
        "   - Paper only shows successes ‚Üí likely cherry-picked\n",
        "   - Paper discusses when it fails ‚Üí more honest\n",
        "\n",
        "**Deep evaluation (if passes filter):**\n",
        "\n",
        "5. **Test on YOUR specific problem:**\n",
        "   - Don't trust benchmarks blindly\n",
        "   - Run 100-200 examples from your domain\n",
        "   - Measure what matters: task success, latency, cost\n",
        "\n",
        "6. **Integration cost estimate:**\n",
        "   - Drop-in replacement (change model name) ‚Üí 1 day\n",
        "   - New architecture (custom code) ‚Üí 1-2 weeks\n",
        "   - Custom training pipeline ‚Üí 1 month+\n",
        "\n",
        "7. **Maintenance cost:**\n",
        "   - Will this break with library updates?\n",
        "   - Does team have expertise to debug?\n",
        "   - Is there community support?\n",
        "\n",
        "**Real examples:**\n",
        "\n",
        "‚úÖ **Worth it:** FlashAttention (2022)\n",
        "- Massive speedup (2-4x)\n",
        "- Drop-in replacement\n",
        "- Widely adopted, good support\n",
        "\n",
        "‚ùå **Not worth it:** Most \"novel architecture\" papers\n",
        "- Marginal gains\n",
        "- Requires custom implementation\n",
        "- No pretrained weights\n",
        "- Team can't maintain\n",
        "\n",
        "#### **\"How do you validate small improvements statistically?\"**\n",
        "\n",
        "**Problem:** Claim \"3% improvement\" but is it real or noise?\n",
        "\n",
        "**Statistical validation:**\n",
        "\n",
        "1. **Sufficient sample size:**\n",
        "   ```\n",
        "   For detecting 3% improvement with 95% confidence:\n",
        "   Need ~1000-1500 samples (depends on variance)\n",
        "   \n",
        "   Too small: 50 samples ‚Üí can't trust 3% difference\n",
        "   ```\n",
        "\n",
        "2. **Bootstrap confidence intervals:**\n",
        "   ```python\n",
        "   # Resample your test set 1000 times\n",
        "   # Calculate metric each time\n",
        "   # 95% CI: [2.5th percentile, 97.5th percentile]\n",
        "   \n",
        "   If improvement CI = [1.5%, 4.5%] ‚Üí significant\n",
        "   If improvement CI = [-0.5%, 6.5%] ‚Üí not significant (crosses 0)\n",
        "   ```\n",
        "\n",
        "3. **Paired testing (critical):**\n",
        "   - Test both models on SAME examples\n",
        "   - Reduces variance, increases statistical power\n",
        "   - Wrong: Model A on 500 samples, Model B on different 500\n",
        "   - Right: Both models on same 500 samples\n",
        "\n",
        "4. **Multiple metrics:**\n",
        "   ```\n",
        "   Don't just report one metric where you won\n",
        "   \n",
        "   Report:\n",
        "   - Primary metric (task success)\n",
        "   - Quality metrics (relevance, coherence)\n",
        "   - Efficiency metrics (latency, cost)\n",
        "   - Failure modes (refusal rate, hallucination)\n",
        "   ```\n",
        "\n",
        "5. **Significance testing:**\n",
        "   - **T-test** (if metrics are normally distributed)\n",
        "   - **Mann-Whitney U** (if not normal, e.g., ratings 1-5)\n",
        "   - **p-value < 0.05** = significant\n",
        "   - But also check **effect size** (Cohen's d)\n",
        "\n",
        "**Red flags:**\n",
        "- Only tested on 50 examples ‚Üí too small\n",
        "- Only reports improvement on 1 cherry-picked metric\n",
        "- No confidence intervals or p-values\n",
        "- \"Improvement\" smaller than measurement noise\n",
        "\n",
        "**Practical rule:**\n",
        "```\n",
        "Claim 3% improvement as real only if:\n",
        "‚úì Tested on 1000+ samples\n",
        "‚úì p-value < 0.05\n",
        "‚úì Confidence interval doesn't include 0\n",
        "‚úì Consistent across multiple metrics\n",
        "‚úì Holds on diverse subsets of data\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **13. Multi-Agent & Tooling**\n",
        "\n",
        "#### **\"When are agents overkill?\"**\n",
        "\n",
        "**Use single LLM call when:**\n",
        "- Task is simple, one-step (Q&A, summarization)\n",
        "- Latency critical (< 1s response needed)\n",
        "- Limited budget (each agent step costs $$$)\n",
        "- Failure modes hard to debug\n",
        "\n",
        "**Use agents when:**\n",
        "- Task requires multiple steps (search ‚Üí analyze ‚Üí format)\n",
        "- Steps are conditional (if X then do Y, else Z)\n",
        "- Need external tools (search, calculator, database)\n",
        "- Correctness > latency\n",
        "\n",
        "**Real examples:**\n",
        "\n",
        "‚ùå **Agent overkill:**\n",
        "- Simple customer support FAQ\n",
        "- Basic document summarization\n",
        "- Single-step classification\n",
        "\n",
        "‚úÖ **Agents make sense:**\n",
        "- Complex research tasks (search multiple sources ‚Üí synthesize)\n",
        "- Data analysis (query DB ‚Üí analyze ‚Üí visualize)\n",
        "- Multi-step workflows (validate input ‚Üí process ‚Üí verify output)\n",
        "\n",
        "**Cost comparison:**\n",
        "```\n",
        "Single call: $0.01\n",
        "Agent (3 steps): $0.03-0.05\n",
        "Agent (10 steps): $0.10-0.20\n",
        "\n",
        "If 80% of queries can be solved in 1 call ‚Üí don't use agent\n",
        "```\n",
        "\n",
        "#### **\"Design a tool-calling system safely\"**\n",
        "\n",
        "**Core safety challenges:**\n",
        "1. Model calls wrong tool\n",
        "2. Model generates invalid parameters\n",
        "3. Model stuck in infinite loop\n",
        "4. Tool returns error, model doesn't handle it\n",
        "\n",
        "**Safe design (layered defense):**\n",
        "\n",
        "**Layer 1: Constrain tool choice**\n",
        "```python\n",
        "# Don't give model all tools at once\n",
        "# Give only relevant tools based on query\n",
        "\n",
        "if \"weather\" in query:\n",
        "    tools = [weather_api]\n",
        "elif \"calculation\" in query:\n",
        "    tools = [calculator]\n",
        "else:\n",
        "    tools = [search]  # safe default\n",
        "```\n",
        "\n",
        "**Layer 2: Validate parameters**\n",
        "```python\n",
        "def safe_tool_call(tool, params):\n",
        "    # Validate before calling\n",
        "    if tool == \"database_query\":\n",
        "        if not validate_sql(params[\"query\"]):\n",
        "            return \"Invalid SQL, please retry\"\n",
        "        if is_destructive(params[\"query\"]):  # DELETE, DROP\n",
        "            return \"Destructive queries not allowed\"\n",
        "    \n",
        "    # Sandbox execution\n",
        "    try:\n",
        "        result = tool.execute(params, timeout=5)\n",
        "    except TimeoutError:\n",
        "        return \"Tool execution timeout\"\n",
        "    \n",
        "    return result\n",
        "```\n",
        "\n",
        "**Layer 3: Limit iterations**\n",
        "```python\n",
        "MAX_STEPS = 5\n",
        "\n",
        "for step in range(MAX_STEPS):\n",
        "    action = model.generate(prompt)\n",
        "    if action == \"FINAL_ANSWER\":\n",
        "        break\n",
        "    result = safe_tool_call(action.tool, action.params)\n",
        "    prompt += result\n",
        "\n",
        "if step == MAX_STEPS - 1:\n",
        "    return \"Could not complete task in allowed steps\"\n",
        "```\n",
        "\n",
        "**Layer 4: Human-in-the-loop for risky actions**\n",
        "```python\n",
        "RISKY_TOOLS = [\"send_email\", \"charge_payment\", \"delete_data\"]\n",
        "\n",
        "if action.tool in RISKY_TOOLS:\n",
        "    # Show user confirmation\n",
        "    return f\"About to {action.tool} with {params}. Confirm?\"\n",
        "```\n",
        "\n",
        "**Layer 5: Audit logging**\n",
        "```python\n",
        "# Log every tool call\n",
        "log({\n",
        "    \"timestamp\": now(),\n",
        "    \"user_id\": user_id,\n",
        "    \"query\": query,\n",
        "    \"tool\": tool_name,\n",
        "    \"params\": params,\n",
        "    \"result\": result,\n",
        "    \"model_reasoning\": reasoning\n",
        "})\n",
        "```\n",
        "\n",
        "#### **\"How do you prevent tool hallucinations?\"**\n",
        "\n",
        "**What is tool hallucination?**\n",
        "```\n",
        "User: \"What's the weather in Paris?\"\n",
        "Model: \"I'll call weather_api(city='Paris')\"\n",
        "Model: [fabricates] \"The API returned: 72¬∞F and sunny\"\n",
        "       ‚Üë Model made this up, didn't actually call API\n",
        "```\n",
        "\n",
        "**Prevention strategies:**\n",
        "\n",
        "1. **Structured output enforcement:**\n",
        "```python\n",
        "# Force model to output JSON, parse strictly\n",
        "response = model.generate(\n",
        "    prompt,\n",
        "    response_format={\"type\": \"json_object\"}\n",
        ")\n",
        "tool_call = json.loads(response)  # Fails if not valid JSON\n",
        "\n",
        "# Then YOU execute the tool, model doesn't report results\n",
        "actual_result = execute_tool(tool_call)\n",
        "```\n",
        "\n",
        "2. **Separate generation from execution:**\n",
        "```python\n",
        "# Step 1: Model generates plan\n",
        "plan = model.generate(\"What tools do you need?\")\n",
        "\n",
        "# Step 2: YOU execute tools\n",
        "results = []\n",
        "for tool in plan.tools:\n",
        "    result = actually_call_api(tool)\n",
        "    results.append(result)\n",
        "\n",
        "# Step 3: Model synthesizes (with real results injected)\n",
        "answer = model.generate(f\"Based on results: {results}, answer:\")\n",
        "```\n",
        "\n",
        "3. **Add result verification:**\n",
        "```python\n",
        "# Model must reference specific result fields\n",
        "system_prompt = \"\"\"\n",
        "You MUST cite which tool result you're using:\n",
        "\"According to weather_api result, temperature is {result.temp}¬∞F\"\n",
        "\n",
        "Do NOT make up tool results.\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "4. **Prompt design:**\n",
        "```\n",
        "Bad prompt:\n",
        "\"Use the weather tool and tell the user the result\"\n",
        "‚Üë Model might fabricate result\n",
        "\n",
        "Good prompt:\n",
        "\"First, output: ACTION: weather_api\n",
        "Then wait for system to provide result.\n",
        "Then format the result for user.\"\n",
        "```\n",
        "\n",
        "**Detection (post-hoc):**\n",
        "- Compare model's stated result with actual logged API response\n",
        "- If mismatch ‚Üí log as hallucination, retrain with corrected examples\n",
        "\n",
        "---\n",
        "\n",
        "### **14. RLHF / Alignment (High-Level)**\n",
        "\n",
        "#### **\"Why is PPO expensive in practice?\"**\n",
        "\n",
        "**PPO = Proximal Policy Optimization (used in ChatGPT training)**\n",
        "\n",
        "**Cost breakdown:**\n",
        "\n",
        "1. **Need 4 models loaded simultaneously:**\n",
        "   ```\n",
        "   - Policy model (being trained)        ‚Üí 70B params\n",
        "   - Reference model (frozen baseline)   ‚Üí 70B params\n",
        "   - Reward model (scoring outputs)      ‚Üí 7B params\n",
        "   - Value model (for advantages)        ‚Üí 7B params\n",
        "   \n",
        "   Total: ~220B params in GPU memory\n",
        "   Requires: 8x A100 GPUs minimum\n",
        "   Cost: $20-30/hour\n",
        "   ```\n",
        "\n",
        "2. **Sample inefficiency:**\n",
        "   - Generate responses ‚Üí score them ‚Üí update policy\n",
        "   - Each update uses only 1 batch of data (can't reuse)\n",
        "   - Need millions of samples ‚Üí weeks of training\n",
        "   - Cost: $50K-200K for full RLHF run\n",
        "\n",
        "3. **Reward model training:**\n",
        "   - Need 10K-50K human preference labels\n",
        "   - Labeling cost: $0.50-2 per comparison\n",
        "   - Total: $5K-100K just for labels\n",
        "\n",
        "4. **Hyperparameter sensitivity:**\n",
        "   - PPO is finicky, needs careful tuning\n",
        "   - Many failed runs before finding good config\n",
        "   - Multiply costs by 3-5x for experimentation\n",
        "\n",
        "**Why startups can't afford it:**\n",
        "- Needs dedicated ML infra team\n",
        "- Requires 100K+ training budget\n",
        "- 1-2 month timeline\n",
        "- High risk of failure\n",
        "\n",
        "#### **\"Why startups prefer DPO?\"**\n",
        "\n",
        "**DPO = Direct Preference Optimization**\n",
        "\n",
        "**Why it's cheaper:**\n",
        "\n",
        "1. **Only 1 model needed:**\n",
        "   ```\n",
        "   PPO: 4 models (220B params)\n",
        "   DPO: 1 model (70B params)\n",
        "   \n",
        "   Memory: 4x less\n",
        "   Cost: 4x cheaper\n",
        "   ```\n",
        "\n",
        "2. **Simpler training:**\n",
        "   - No reward model to train separately\n",
        "   - No RL instability issues\n",
        "   - Standard supervised learning pipeline\n",
        "   - Easier to debug\n",
        "\n",
        "3. **Data efficient:**\n",
        "   - Can reuse preference data multiple epochs\n",
        "   - Needs fewer samples than PPO\n",
        "   - 5K-10K preferences often enough (vs 50K+ for PPO)\n",
        "\n",
        "4. **Faster iteration:**\n",
        "   ```\n",
        "   PPO: 2-4 weeks per run\n",
        "   DPO: 2-3 days per run\n",
        "   \n",
        "   More experiments in same budget\n",
        "   ```\n",
        "\n",
        "**Quality comparison:**\n",
        "- DPO often matches PPO quality\n",
        "- Slightly less flexible (can't use complex reward functions)\n",
        "- But for most tasks, good enough\n",
        "\n",
        "**Startup default:** Start with DPO, only consider PPO if:\n",
        "- Have >$100K budget\n",
        "- Need very specific reward shaping\n",
        "- Have ML research team\n",
        "\n",
        "#### **\"When is RLHF not worth it?\"**\n",
        "\n",
        "**Skip RLHF when:**\n",
        "\n",
        "1. **Small data regime:**\n",
        "   - Have < 1000 preference labels\n",
        "   - RLHF needs scale to work\n",
        "   - Better: Few-shot prompting or small SFT\n",
        "\n",
        "2. **Clear objective function:**\n",
        "   ```\n",
        "   Have: Exact match, F1, ROUGE (can compute automatically)\n",
        "   Don't need: Human preferences\n",
        "   \n",
        "   Just do supervised fine-tuning on correct outputs\n",
        "   ```\n",
        "\n",
        "3. **Rapid iteration needed:**\n",
        "   - RLHF takes weeks\n",
        "   - If need to ship in days ‚Üí use prompting\n",
        "\n",
        "4. **Budget <$10K:**\n",
        "   - Can't afford proper RLHF run\n",
        "   - Use synthetic preferences instead (LLM-as-judge)\n",
        "\n",
        "5. **Base model already good:**\n",
        "   ```\n",
        "   GPT-4 / Claude are already RLHF'd\n",
        "   Adding your own RLHF might hurt more than help\n",
        "   \n",
        "   Better: Prompt engineering or small task-specific fine-tune\n",
        "   ```\n",
        "\n",
        "**When RLHF IS worth it:**\n",
        "- Have 10K+ human preferences\n",
        "- Subjective quality matters (helpfulness, tone)\n",
        "- Budget >$50K\n",
        "- Need model to learn nuanced behavior (what humans actually prefer vs what's \"correct\")\n",
        "- Have time (1-2 months)\n",
        "\n",
        "**Middle ground for startups:**\n",
        "```\n",
        "Synthetic RLHF:\n",
        "1. Generate responses from base model\n",
        "2. Use GPT-4 to rank them (simulated preferences)\n",
        "3. Train with DPO on synthetic preferences\n",
        "\n",
        "Cost: $500-2K (100x cheaper)\n",
        "Quality: 70-80% of real RLHF\n",
        "Time: 1 week\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **15. Product Sense (Rare but Powerful)**\n",
        "\n",
        "#### **\"When should you remove an LLM feature?\"**\n",
        "\n",
        "**Kill criteria (any one is enough):**\n",
        "\n",
        "1. **Low usage:**\n",
        "   - < 5% of users use it\n",
        "   - Those who use it, use it < 2x/month\n",
        "   - Indicates: Not solving real problem\n",
        "\n",
        "2. **High support burden:**\n",
        "   - Generates more support tickets than value\n",
        "   - Users constantly confused about how it works\n",
        "   - Cost to support > revenue/value generated\n",
        "\n",
        "3. **Quality ceiling hit:**\n",
        "   - Can't get accuracy above 70%\n",
        "   - Users frustrated by errors\n",
        "   - No clear path to improvement\n",
        "   - Example: Complex math reasoning, current LLMs not good enough\n",
        "\n",
        "4. **Cost unsustainable:**\n",
        "   ```\n",
        "   Monthly feature cost: $5K\n",
        "   Feature revenue: $1K\n",
        "   Burn rate: -$4K/month\n",
        "   \n",
        "   Unless strategic ‚Üí kill it\n",
        "   ```\n",
        "\n",
        "5. **Trust erosion:**\n",
        "   - Feature hallucinated once ‚Üí users lost trust\n",
        "   - Now they don't use ANY LLM features\n",
        "   - One bad feature poisoning the well\n",
        "\n",
        "6. **Better alternatives exist:**\n",
        "   - Users use competitor's feature instead\n",
        "   - Or: Non-LLM solution works better\n",
        "   - Example: Template-based email > LLM-generated\n",
        "\n",
        "**How to make the decision:**\n",
        "```\n",
        "1. Data-driven:\n",
        "   - Usage metrics (DAU, frequency)\n",
        "   - Quality metrics (success rate, user satisfaction)\n",
        "   - Cost per successful interaction\n",
        "\n",
        "2. User research:\n",
        "   - Why aren't people using it?\n",
        "   - What do they use instead?\n",
        "   - Would they miss it if removed?\n",
        "\n",
        "3. Strategic value:\n",
        "   - Does it differentiate us?\n",
        "   - Is it a platform play? (lose money now, strategic later)\n",
        "   - Does it attract users even if not used?\n",
        "\n",
        "Decision: Kill if metrics bad AND no strategic value\n",
        "```\n",
        "\n",
        "**How to sunset gracefully:**\n",
        "- Announce 1 month in advance\n",
        "- Offer migration path (export data, alternative features)\n",
        "- Gather feedback (maybe you misunderstood the problem)\n",
        "\n",
        "#### **\"How do you explain model limitations to PMs?\"**\n",
        "\n",
        "**Bad approach:**\n",
        "\"LLMs hallucinate, it's just how they work\"\n",
        "‚Üí PM hears: \"You can't build reliable products\"\n",
        "\n",
        "**Good approach: Translate to product constraints**\n",
        "\n",
        "```\n",
        "PM asks: \"Can we build automated customer support?\"\n",
        "\n",
        "You say:\n",
        "\"Yes, with guardrails. Here's what's realistic:\n",
        "\n",
        "‚úì Can do (>90% accuracy):\n",
        "  - Answer FAQ questions\n",
        "  - Route tickets to right team\n",
        "  - Summarize long conversations\n",
        "\n",
        "‚ö†Ô∏è Needs review (70-80% accuracy):\n",
        "  - Handle complex multi-step issues\n",
        "  - Interpret ambiguous requests\n",
        "  - Handle edge cases\n",
        "\n",
        "‚úó Can't do reliably (<60% accuracy):\n",
        "  - Financial calculations\n",
        "  - Legal advice\n",
        "  - Guarantee 100% factual accuracy\n",
        "\n",
        "Trade-offs:\n",
        "- Option A: Human-in-the-loop (slower, accurate)\n",
        "- Option B: Fully automated (fast, 10% error rate)\n",
        "- Option C: Hybrid (auto for simple, human for complex)\n",
        "\n",
        "Which aligns with our product goals?\"\n",
        "```\n",
        "\n",
        "**Framework for PM conversations:**\n",
        "\n",
        "1. **Frame in business terms:**\n",
        "   - Not: \"Attention mechanism limitations\"\n",
        "   - But: \"Works for 80% of queries, need fallback for 20%\"\n",
        "\n",
        "2. **Quantify risks:**\n",
        "   - \"95% accuracy means 1 in 20 users see bad output\"\n",
        "   - \"At 10K users/day, that's 500 bad experiences\"\n",
        "\n",
        "3. **Offer solutions, not just problems:**\n",
        "   - \"We can't do X perfectly, but here are 3 approaches with different trade-offs\"\n",
        "\n",
        "4. **Set realistic expectations early:**\n",
        "   - \"This will feel like ChatGPT sometimes, not Google\"\n",
        "   - \"Users will need to verify outputs\"\n",
        "\n",
        "5. **Show what good looks like:**\n",
        "   - Demo on real examples\n",
        "   - Show failure cases too (manage expectations)\n",
        "\n",
        "#### **\"How do you handle user trust after hallucinations?\"**\n",
        "\n",
        "**Scenario:** Your LLM feature gave wrong medical/financial advice, users are upset\n",
        "\n",
        "**Immediate response (24 hours):**\n",
        "\n",
        "1. **Acknowledge publicly:**\n",
        "   ```\n",
        "   \"We're aware Feature X provided incorrect information.\n",
        "   We've temporarily disabled it while we investigate.\n",
        "   We take this seriously.\"\n",
        "   ```\n",
        "   \n",
        "2. **Immediate safety measures:**\n",
        "   - Add disclaimer: \"Verify critical information\"\n",
        "   - Add confidence scores (if low, show warning)\n",
        "   - Human review for high-stakes domains\n",
        "\n",
        "3. **Root cause:**\n",
        "   - Was it retrieval failure? (wrong context)\n",
        "   - Model hallucination? (fabricated facts)\n",
        "   - Edge case? (input type never tested)\n",
        "\n",
        "**Medium-term (1-2 weeks):**\n",
        "\n",
        "4. **Product changes:**\n",
        "   ```\n",
        "   Before: Direct answer\n",
        "   After: Answer + sources + confidence + \"Verify if critical\"\n",
        "   \n",
        "   Before: Auto-execute actions\n",
        "   After: Show preview, require confirmation\n",
        "   ```\n",
        "\n",
        "5. **Evaluation upgrade:**\n",
        "   - Add adversarial test cases\n",
        "   - Red-team the feature (try to break it)\n",
        "   - Add monitoring for high-stakes queries\n",
        "\n",
        "6. **Communication:**\n",
        "   - Explain what went wrong (transparently)\n",
        "   - What you changed\n",
        "   - How you're preventing it\n",
        "\n",
        "**Long-term (ongoing):**\n",
        "\n",
        "7. **Rebuild trust:**\n",
        "   - Show accuracy metrics publicly\n",
        "   - User controls (toggle features on/off)\n",
        "   - Easy reporting (thumbs down, \"this is wrong\" button)\n",
        "   - Show you're taking feedback seriously\n",
        "\n",
        "8. **Product positioning:**\n",
        "   - Not: \"AI that knows everything\"\n",
        "   - But: \"AI assistant that helps, but you stay in control\"\n",
        "\n",
        "9. **Domain-specific boundaries:**\n",
        "   ```\n",
        "   Medical: \"Not medical advice, consult doctor\"\n",
        "   Legal: \"Not legal advice, consult lawyer\"\n",
        "   Financial: \"Not financial advice, DYOR\"\n",
        "   \n",
        "   + Technical measures (refuse certain queries)\n",
        "   ```\n",
        "\n",
        "**What NOT to do:**\n",
        "- ‚ùå Blame the user (\"You should have known\")\n",
        "- ‚ùå Blame the technology (\"All LLMs do this\")\n",
        "- ‚ùå Over-promise fixes (\"Will never happen again\")\n",
        "- ‚ùå Hide the incident (users remember, trust erodes)\n",
        "\n",
        "**Key principle:**\n",
        "\"Trust is built slowly, lost quickly, rebuilt even slower\"\n",
        "‚Üí Over-invest in safety for high-stakes domains\n",
        "\n",
        "---\n",
        "\n",
        "### **Quick Recall Bullets for Part 4:**\n",
        "\n",
        "‚úÖ **Research paper validation:** Reproduce ‚Üí test on YOUR data ‚Üí measure engineering cost ‚Üí ship only if >10% gain  \n",
        "‚úÖ **Statistical significance:** Need 1000+ samples, bootstrap CI, paired testing, p< 0.05  \n",
        "‚úÖ **Agents:** Overkill for simple tasks, use when multi-step + conditional logic needed  \n",
        "‚úÖ **Tool safety:** Validate params, limit iterations, sandbox execution, audit log  \n",
        "‚úÖ **Tool hallucination:** Separate generation from execution, YOU call tools, inject real results  \n",
        "‚úÖ **PPO vs DPO:** PPO 4x more expensive, needs 4 models; DPO simpler, 1 model, startups prefer it  \n",
        "‚úÖ **RLHF not worth it:** When < 1K labels, clear objective exists, budget <$10K, rapid iteration needed  \n",
        "‚úÖ **Kill LLM feature:** Low usage + high cost + quality ceiling + trust erosion  \n",
        "‚úÖ **Explain to PMs:** Use business terms, quantify risks, offer trade-offs, set realistic expectations  \n",
        "‚úÖ **Trust after hallucination:** Acknowledge, add safeguards, explain transparently, rebuild slowly  \n",
        "\n",
        "---\n",
        "\n",
        "## üéØ FINAL SUMMARY: TOP 20 MUST-KNOW POINTS\n",
        "\n",
        "If you remember only these, you'll handle 80% of interviews:\n",
        "\n",
        "### Design & Architecture\n",
        "1. **RAG vs Fine-tuning:** RAG for changing knowledge, FT for style/cost at scale\n",
        "2. **Evaluation 3-tier:** Automated (LLM-judge) ‚Üí Human spot-check ‚Üí Production metrics\n",
        "3. **Model choice:** API first, open-source at scale ($5K+/mo)\n",
        "\n",
        "### Debugging & Failure Modes\n",
        "4. **RAG debug order:** Retrieval ‚Üí chunk quality ‚Üí prompt strength ‚Üí context position\n",
        "5. **Hallucination fix:** Stronger instruction + few-shot + post-filter\n",
        "6. **Test ‚â† Prod:** Always build adversarial test set from real failures\n",
        "\n",
        "### Performance & Cost\n",
        "7. **Latency debug:** Log each component ‚Üí find bottleneck ‚Üí optimize that\n",
        "8. **Token cost control:** Sliding window + summarize + hard caps + route to cheaper models\n",
        "9. **KV-cache:** Critical for multi-turn, saves recomputation, uses more memory\n",
        "\n",
        "### Optimization\n",
        "10. **vLLM for production:** >50 req/min needs vLLM/TGI, not HuggingFace\n",
        "11. **Quantization rules:** INT8 for 7B+, INT4 only for 70B+, calibrate on prod data\n",
        "12. **Bursty traffic:** Queue + rate limit + auto-scale, monitor P95/P99 latency\n",
        "\n",
        "### Model Updates\n",
        "13. **Safe rollout:** Shadow ‚Üí Canary (5%) ‚Üí Gradual ‚Üí Keep old model warm 48h\n",
        "14. **Rollback:** Feature flags, instant switch, keep old model running\n",
        "15. **Compare models:** LLM-judge + human pairwise + production metrics + statistical significance\n",
        "\n",
        "### Advanced Topics\n",
        "16. **Agents:** Use only for multi-step conditional workflows, overkill for simple tasks\n",
        "17. **Tool safety:** Validate params, limit iterations, YOU execute tools (prevent hallucination)\n",
        "18. **DPO vs PPO:** DPO 4x cheaper, simpler, good enough for startups\n",
        "19. **Research papers:** Test on YOUR data, ship only if >10% gain, consider engineering cost\n",
        "\n",
        "### Product Thinking\n",
        "20. **Interview mindset:** Clarify constraints ‚Üí Simple baseline ‚Üí Failure modes ‚Üí Iteration plan\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "pQDE_kxjM6-k"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B6HIqsHOTEXQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}