{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0dBvm8EWpYr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Theory -1"
      ],
      "metadata": {
        "id": "aGyRqVnTZp7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Mechanisms"
      ],
      "metadata": {
        "id": "tCNDBTzu7AkQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. What is the function of Q,K,V in Attention**\n",
        "\n",
        "Consider the sentence:\n",
        "**The bank of the river was muddy**\n",
        "\n",
        "\n",
        "**The \"Search\" (Query)** : The word **\"bank\"** sends out a Query: \"I am a noun, but I need context. Am I related to money or geography? Look for words that describe my surroundings.\"\n",
        "\n",
        "**The \"Match\" (Key comparison)**:\n",
        "The Query from \"bank\" compares itself against the Keys of every other word:\n",
        "\n",
        "    \"The\": Key says \"I am a generic article.\" â†’ Match: Low.\n",
        "    \"River\": Key says \"I am a body of water, geography, nature.\" â†’ Match: Very High.\n",
        "    \"Muddy\": Key says \"I am a texture, dirt, related to earth.\" â†’ Match: High.\n",
        "\n",
        "**The \"Weights\" (Attention Scores) : Softmax**\n",
        "The **\"River\"** Key matches the **\"geographic\"** part of the **\"Bank\"** Query perfectly. Because of this high match, the model assigns a high **```Attention Score (e.g., 0.85)```** to the word \"river.\" and low sore to rest of the words\n",
        "\n",
        "**The \"Retrieval\" (Value merging)**:\n",
        "The model now takes all the Value (the actual data/meaning) of each word and multiplies them by their scores. The controbution of **\"river\"** in determining the nature of **\"bank\"** is so high rest can be ignored.\n",
        "\n",
        "**The \"New Meaning\" (Context Awareness)**\n",
        "By the end of this layer, the mathematical vector for \"bank\" is no longer just the generic word. It has been \"pulled\" toward the meaning of \"river.\"\n",
        "\n",
        "But for this sentence **The bank closed my account** Query from **\"bank\"** would have found a high match with the Key for **\"account\"** instead.\n",
        "\n",
        "**2. Multi-Head Attention vs Single-Head Attention**\n",
        "\n",
        "**Why multiple heads?** Each head learns different relationship patterns in separate representation subspaces. Head 1 might capture syntax, Head 2 semantics, Head 3 long-range dependencies.\n",
        "\n",
        "**Math:** Instead of one $d_{model} \\times d_{model}$ attention, $d_{model}$ split into $h$ heads of size $d_k = d_{model}/h$\n",
        "\n",
        "Final output: $\\text{Concat}(head_1, ..., head_h)W^O$\n",
        "\n",
        "**Scaling factor:** $\\frac{1}{\\sqrt{d_k}}$ prevents dot products from growing too large, which would push softmax into saturation regions with tiny gradients.\n",
        "\n",
        "***\n",
        "\n",
        "**3. Multi-Query Attention (MQA) vs Multi-Head Attention (MHA)**\n",
        "\n",
        "**Why share K/V?** In MHA, each head has separate $W^K, W^V$ matrices. MQA uses **one shared K/V projection** for all heads, only queries stay separate.\n",
        "\n",
        "**Memory savings:** KV-cache size drops from $2 \\times L \\times h \\times d_k \\times n$ to $2 \\times L \\times d_k \\times n$ (~8x reduction for 8 heads).\n",
        "\n",
        "$2$ -> both ```Keys & Values``` vector is stored for each token\n",
        "\n",
        "$L$ -> no. of ```Layers``` in transformer (32 layers in Llama-2-7B)\n",
        "\n",
        "$h$ -> ```Heads``` each attention head has its own unique K and V projections\n",
        "\n",
        "$d_k$ -> ```Head Dimension``` for single head in **MHA**\n",
        "\n",
        "$n$  -> ```Sequence Length```: no. of tokens currently in context(prompt + generated tokens)\n",
        "\n",
        "**Trade-off:** Slight quality drop (~1-2% on benchmarks) but 4-5x faster inference due to reduced memory bandwidth.\n",
        "\n",
        "***\n",
        "\n",
        "**4. Grouped Query Attention (GQA)**\n",
        "\n",
        "**Why groups?** Middle ground between MHA (h separate KV pairs) and MQA (1 shared KV pair). Use $g$ groups, e.g., 8 heads share 2 KV pairs (4 heads per group).\n",
        "\n",
        "**Math:** If $h=8, g=2$\n",
        "\n",
        "heads 1-4 share KVâ‚, heads 5-8 share KVâ‚‚.\n",
        "\n",
        "**Optimal:** LLaMA 2 70B uses $h=64, g=8$ (8 heads per group) for balance.\n",
        "\n",
        "***\n",
        "\n",
        "### Flash Attention\n",
        "**Why $O(n^2)$ memory?** Standard attention computes full $n \\times n$ attention matrix before softmax.\n",
        "\n",
        "**Fix:** Tile the computation into blocks, load from ```HBM â†’ SRAM â†’ compute â†’ write back```. Never materialize full matrix.\n",
        "\n",
        "**Result:** Memory drops from $O(n^2)$ to $O(n)$ while being 2-4x faster via kernel fusion.\n",
        "\n",
        "***\n",
        "\n",
        "### Sliding Window Attention\n",
        "**Why limit window?** Mistral uses 4096-token sliding window. Each token only attends to previous 4096 tokens, not full sequence.\n",
        "\n",
        "**Math:** Attention mask $M_{ij} = 0$ if $i - j > 4096$, else 1.\n",
        "\n",
        "**Gradient flow:** Still $O(n^2)$ theoretically, but practical memory and compute are $O(n \\cdot w)$ where $w=4096$.\n",
        "\n",
        "**Loss:** Can't directly reference tokens >4096 positions back, but multi-layer stacking allows indirect access (layer 2 sees layer 1's window, so effectively 2Ã— window depth).\n",
        "\n",
        "***\n",
        "\n",
        "## Positional Encodings\n",
        "\n",
        "### Sinusoidal vs Learned Embeddings\n",
        "**Sinusoidal formula:**\n",
        "\n",
        "$PE_{(pos, 2i)} = \\sin(pos/10000^{2i/d})$,\n",
        "$PE_{(pos, 2i+1)} = \\cos(pos/10000^{2i/d})$.\n",
        "\n",
        "**Why?** Allows extrapolation: $PE_{pos+k}$ can be expressed as linear function of $PE_{pos}$\n",
        "\n",
        "**Learned:** Train $W_{pos} \\in \\mathbb{R}^{max\\_len \\times d}$. Fails at test-time lengths > training max_len.\n",
        "\n",
        "**Modern choice:** Neither. Use RoPE or Alibi instead.\n",
        "\n",
        "***\n",
        "\n",
        "### Rotary Position Embedding (RoPE)\n",
        "**Core idea:** Rotate query/key vectors by angle proportional to position.\n",
        "\n",
        "**Math:** For position $m$, apply rotation matrix $R_m$:\n",
        "$$\n",
        "f(q, m) = \\begin{bmatrix} \\cos(m\\theta) & -\\sin(m\\theta) \\\\ \\sin(m\\theta) & \\cos(m\\theta) \\end{bmatrix} q\n",
        "$$\n",
        "\n",
        "**Key property:** $q_m^T k_n = (R_m q)^T (R_n k) = q^T R_{n-m} k$\n",
        "\n",
        "dot product depends only on **relative distance** $n-m$\n",
        "\n",
        "**Used in:** LLaMA, GPT-NeoX, PaLM.\n",
        "\n",
        "***\n",
        "\n",
        "### Alibi (Attention with Linear Biases)\n",
        "**Formula:** Add bias $-m \\cdot |i-j|$ to attention scores, where $m$ is head-specific slope.\n",
        "\n",
        "**Math:** $\\text{softmax}(QK^T + \\text{Alibi})$,\n",
        "\n",
        "Alibi matrix is $[-m, -2m, -3m, ...]$ for each row.\n",
        "\n",
        "**Advantage:** No positional embeddings needed. Extrapolates better to longer sequences than RoPE in some tasks.\n",
        "\n",
        "**Used in:** BLOOM, MPT.\n",
        "\n",
        "***\n",
        "\n",
        "## Normalization\n",
        "\n",
        "### Pre-Norm vs Post-Norm\n",
        "**Post-norm (original Transformer):**\n",
        "$x = \\text{LN}(x + \\text{Attention}(x))$\n",
        "\n",
        "**Pre-norm (modern):**\n",
        "$x = x + \\text{Attention}(\\text{LN}(x))$\n",
        "\n",
        "**Why pre-norm?** Gradients flow directly through residual path without passing through normalization. Stabilizes training in deep models (>24 layers). GPT-2/3, LLaMA all use pre-norm.\n",
        "\n",
        "***\n",
        "\n",
        "### RMSNorm vs LayerNorm\n",
        "**LayerNorm:**\n",
        "$y = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta$\n",
        "\n",
        "**RMSNorm (removes mean centering):**\n",
        "$y = \\frac{x}{\\sqrt{\\frac{1}{d}\\sum x_i^2 + \\epsilon}} \\cdot \\gamma$\n",
        "\n",
        "**Speedup:** 10-30% faster (single pass vs two-pass), less memory.\n",
        "\n",
        "**Why it works:** Re-centering contributes little to transformer performance; rescaling alone suffices.[5]\n",
        "\n",
        "***\n",
        "\n",
        "## Activation Functions\n",
        "\n",
        "### SwiGLU vs GELU\n",
        "**GELU:** $\\text{GELU}(x) = x \\cdot \\Phi(x)$ where $\\Phi$ is Gaussian CDF.\n",
        "\n",
        "**SwiGLU:**\n",
        "$\n",
        "\\text{SwiGLU}(x, W, V) = (Wx \\odot \\sigma(Vx))\n",
        "$\n",
        "\n",
        "where $\\sigma$ is Swish ($x \\cdot \\text{sigmoid}(x)$), $\\odot$ is element-wise product.\n",
        "\n",
        "**Why gating?** The $\\odot$ operation acts as a learned filterâ€”decides which activations to pass through.\n",
        "\n",
        "**Performance:** LLaMA uses SwiGLU, shows ~1% improvement over GELU.\n",
        "\n",
        "***\n",
        "\n",
        "### Squared ReLU\n",
        "**Formula:** $\\text{ReLU}^2(x) = \\max(0, x)^2$.\n",
        "\n",
        "**Gradient:** $\\frac{d}{dx} = 2x$ for $x > 0$, stronger gradient signal than linear ReLU.\n",
        "\n",
        "**Usage:** Some Primer models, less common than SwiGLU.\n",
        "\n",
        "***\n",
        "\n",
        "## Feed-Forward Networks\n",
        "\n",
        "### FFN Expansion Ratio\n",
        "**Standard:** $d_{ff} = 4 \\times d_{model}$ (e.g., 768 â†’ 3072 in BERT).\n",
        "\n",
        "**LLaMA:** Uses $8 \\times d_{model}$ (4096 â†’ 32768 for 7B model).\n",
        "\n",
        "**Why?** More parameters in FFN = better memorization capacity. Trade-off: 2Ã— FLOPs per layer.\n",
        "\n",
        "**Parameter split:** ```In transformers, ~â…” of parameters are in FFN layers.```\n",
        "\n",
        "***\n",
        "\n",
        "### Mixture of Experts (MoE)\n",
        "**Formula:**\n",
        "$\n",
        "y = \\sum_{i=1}^{N} G(x)_i \\cdot E_i(x)\n",
        "$\n",
        "\n",
        "where $G(x) = \\text{TopK}(\\text{softmax}(Wx))$ is gating network, $E_i$ are expert FFNs.\n",
        "\n",
        "**Example:** Mixtral 8Ã—7B has 8 experts, activates top-2 per token. Total params: 47B, active params per token: 13B.\n",
        "\n",
        "**Load balancing loss:**\n",
        "$\n",
        "L_{aux} = \\alpha \\sum_{i=1}^{N} f_i \\cdot P_i\n",
        "$\n",
        "\n",
        "where $f_i$ = fraction of tokens to expert $i$, $P_i$ = avg gate probability for expert $i$. Prevents all tokens routing to one expert.\n",
        "\n",
        "***\n",
        "\n",
        "## Tokenization\n",
        "\n",
        "### BPE vs WordPiece\n",
        "**BPE:** Greedy merging of most frequent byte pairs. ```\"hugging\" â†’ \"hug\" + \"ging\"```.\n",
        "\n",
        "**WordPiece:** Maximizes likelihood $P(corpus)$ when choosing merges instead of raw frequency.\n",
        "\n",
        "**Vocab size:** GPT-2 uses 50k BPE, BERT uses 30k WordPiece.\n",
        "\n",
        "**Compression:** ```Larger vocab â†’ shorter sequences â†’ faster training, but larger embedding matrix.```\n",
        "\n",
        "***\n",
        "\n",
        "### Byte-Level BPE\n",
        "**Change:** Operate on UTF-8 bytes (256 base tokens) instead of Unicode characters.\n",
        "\n",
        "**Benefit:** **Zero UNK tokens**â€”any text can be encoded (even emojis, rare scripts).\n",
        "\n",
        "**Used in:** GPT-2, GPT-3, LLaMA.\n",
        "\n",
        "***\n",
        "\n",
        "## Architecture Variants\n",
        "\n",
        "### Decoder-Only vs Encoder-Decoder\n",
        "**Decoder-only (GPT):** Causal attention mask (can't see future). Autoregressive generation.\n",
        "\n",
        "**Encoder-decoder (T5):** Encoder uses bidirectional attention, decoder uses causal. Better for seq2seq (translation).\n",
        "\n",
        "**Why decoder-only dominates?** Simpler architecture, scales better, instruction-following emerges naturally with causal LM objective.\n",
        "\n",
        "***\n",
        "\n",
        "## Training Techniques\n",
        "\n",
        "### Gradient Checkpointing\n",
        "**Standard:** Store all activations during forward pass (memory = $O(L \\cdot B \\cdot d)$).\n",
        "\n",
        "**Checkpointing:** Store only every $k$-th layer, recompute intermediate activations during backward.\n",
        "\n",
        "**Trade-off:** ~30% slower training, but 10Ã— memory reduction. Enables larger batch sizes.\n",
        "\n",
        "***\n",
        "\n",
        "### BFloat16 vs Float16\n",
        "**Float16:** 1 sign, 5 exp, 10 mantissa. Range: $\\pm 6.5 \\times 10^4$.\n",
        "\n",
        "**BFloat16:** 1 sign, **8 exp**, 7 mantissa. Range: $\\pm 3.4 \\times 10^{38}$ (same as FP32).\n",
        "\n",
        "**Why BF16?** No loss scaling needed (wide exponent range). Used in TPUs, A100 GPUs.\n",
        "\n",
        "***\n",
        "\n",
        "### Gradient Accumulation\n",
        "**Formula:** Accumulate gradients over $N$ microbatches before optimizer step.\n",
        "\n",
        "**Effective batch:** $B_{eff} = N \\times B_{micro}$.\n",
        "\n",
        "**Memory:** Only $B_{micro}$ activations in memory at once, but same gradient as $B_{eff}$.\n",
        "\n",
        "**Drawback:** BatchNorm breaks (but transformers use LayerNorm, so irrelevant).\n",
        "\n",
        "***\n",
        "\n",
        "## Fine-Tuning Methods\n",
        "\n",
        "### LoRA\n",
        "**Formula:** Freeze $W \\in \\mathbb{R}^{d \\times k}$, train $\\Delta W = BA$\n",
        "\n",
        "where $B \\in \\mathbb{R}^{d \\times r}, A \\in \\mathbb{R}^{r \\times k}$\n",
        "\n",
        "**Forward:** $h = Wx + BAx$\n",
        "\n",
        "**Parameters:** Reduce from $d \\times k$ to $r(d+k)$.\n",
        "\n",
        "For $r=8, d=4096, k=4096$: 16M â†’ 65k (250Ã— reduction).\n",
        "\n",
        "**Why low-rank?** Hypothesis: weight updates during fine-tuning lie in low-dimensional subspace.[9][10]\n",
        "\n",
        "***\n",
        "\n",
        "### QLoRA\n",
        "**Change:** Load base model in 4-bit (NF4 quantization), train LoRA adapters in FP16.\n",
        "\n",
        "**Memory:** 7B model fits in 5GB VRAM (vs 28GB for FP32).\n",
        "\n",
        "**Double quantization:** Quantize the quantization constants themselves (saves 0.4 bits per param).\n",
        "\n",
        "***\n",
        "\n",
        "### Prefix Tuning vs LoRA\n",
        "**Prefix tuning:** Prepend $k$ learnable vectors to K/V at each layer.\n",
        "\n",
        "**Parameters:** $2 \\times L \\times k \\times d$ (e.g., 2 Ã— 24 Ã— 20 Ã— 768 = 737k for BERT).\n",
        "\n",
        "**LoRA:** Modifies weight matrices directly, more parameter-efficient for same performance.\n",
        "\n",
        "***\n",
        "\n",
        "## Inference Optimization\n",
        "\n",
        "### KV-Cache\n",
        "**Without cache:** Generate token $t$, must recompute attention for tokens $1...t-1$.\n",
        "\n",
        "**With cache:** Store $K_{1:t-1}, V_{1:t-1}$, only compute $K_t, V_t$.\n",
        "\n",
        "Attention: $Q_t \\cdot [K_{1:t}]$.\n",
        "\n",
        "**Memory:** $2 \\times L \\times h \\times d_k \\times n$ floats. For LLaMA 7B at 2048 tokens: ~2GB.\n",
        "\n",
        "***\n",
        "\n",
        "### Speculative Decoding\n",
        "**Idea:** Small model (160M) generates $$k$$ tokens speculatively, large model (7B) verifies in parallel.\n",
        "\n",
        "**Acceptance:** If $P_{large}(x_i) \\geq P_{small}(x_i)$, accept. Else, resample.\n",
        "\n",
        "**Speedup:** 2-3Ã— for greedy decoding when draft model accuracy >70%.\n",
        "\n",
        "***\n",
        "\n",
        "### GPTQ Quantization\n",
        "**Method:** Use second-order Hessian information to minimize quantization error layer-by-layer.\n",
        "\n",
        "**Formula:** Minimize $||WX - \\hat{W}X||^2_F$ subject to $\\hat{W}$ being quantized.\n",
        "\n",
        "**Result:** 4-bit quantization with < 1% perplexity increase.\n",
        "\n",
        "***\n"
      ],
      "metadata": {
        "id": "AlsX4gRlWqeH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPTQ Quantization\n",
        "**Method:** Use second-order Hessian information to minimize quantization error layer-by-layer.\n",
        "\n",
        "**Formula:** Minimize $||WX - \\hat{W}X||^2_F$ subject to $\\hat{W}$ being quantized.\n",
        "\n",
        "**Result:** 4-bit quantization with < 1% perplexity increase.\n",
        "\n",
        "***\n",
        "\n",
        "## Loss Functions\n",
        "\n",
        "### Cross-Entropy with Label Smoothing\n",
        "**Standard:** $L = -\\log p_{true}$.\n",
        "\n",
        "**Smoothed:** $L = -(1-\\epsilon)\\log p_{true} - \\epsilon \\sum_{i \\neq true} \\log p_i / (C-1)$.\n",
        "\n",
        "**Effect:** Prevent overconfident predictions (softmax â†’ 1.0), regularizes model.\n",
        "\n",
        "***\n",
        "\n",
        "### Next Token Prediction vs MLM\n",
        "**Causal LM:** $p(x_t | x_{1:t-1})$. Must predict sequentially.\n",
        "\n",
        "**MLM (BERT):** $p(x_t | x_{\\neq t})$. Can see full context, but can't generate autoregressively.\n",
        "\n",
        "**Why causal wins:** Unified pretraining + generation. Instruction-following is just conditional generation.\n",
        "\n",
        "***"
      ],
      "metadata": {
        "id": "OQhEx_SHSXY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Theory -2"
      ],
      "metadata": {
        "id": "AzP94PkQZ2s1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Weight Initialization\n",
        "\n",
        "### Xavier/Glorot vs He Initialization\n",
        "**Xavier (Glorot):** $W \\sim \\mathcal{U}\\left[-\\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}}, \\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}}\\right]$\n",
        "\n",
        "**He:** $W \\sim \\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{\\text{in}}}}\\right)$\n",
        "\n",
        "**Key difference:** Xavier designed for tanh/sigmoid (symmetric around 0). He for ReLU, which zeros out half the gradientsâ€”hence factor of 2 instead of 6 to compensate.\n",
        "\n",
        "**Rule of thumb:** Use Xavier for attention projections (mostly linear). Use He for FFN layers (followed by ReLU/GELU).\n",
        "\n",
        "**Embeddings:** Typically $\\mathcal{N}(0, 0.02^2)$ â€” small enough to avoid saturation early in training.\n",
        "\n",
        "---\n",
        "\n",
        "### LayerNorm: Why Î³=1, Î²=0?\n",
        "**At init:** $\\text{LN}(x) = \\frac{x - \\mu}{\\sigma}$ (pure normalization, no learned transform)\n",
        "\n",
        "**During training:** Network learns optimal $\\gamma$ (scale) and $\\beta$ (shift) per dimension.\n",
        "\n",
        "**Why start at identity?** Stable gradients from step 1. Network decides what rescaling it needs, rather than fighting random initial scales.\n",
        "\n",
        "---\n",
        "\n",
        "## Attention Mechanics\n",
        "\n",
        "### Softmax Temperature Scaling\n",
        "**Standard:** $\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)$\n",
        "\n",
        "**With temperature:** $\\text{softmax}\\left(\\frac{QK^T}{T \\cdot \\sqrt{d_k}}\\right)$\n",
        "\n",
        "**Effect:**\n",
        "- $T > 1$: Flatter distribution â†’ more token diversity\n",
        "- $T < 1$: Sharper peaks â†’ focus on top matches\n",
        "- Some models learn $T$ per head (init ~0.5)\n",
        "\n",
        "**Mental model:** Temperature is like \"attention confidence.\" High T = \"I'm uncertain, consider everything.\" Low T = \"I know what's important.\"\n",
        "\n",
        "---\n",
        "\n",
        "### Causal Masking\n",
        "**Implementation:** Set $M_{ij} = -\\infty$ where $j > i$ before softmax\n",
        "\n",
        "**Result:** $\\text{softmax}([-\\infty, -\\infty, 2.1, 3.5]) = [0, 0, 0.25, 0.75]$\n",
        "\n",
        "**Why at inference?** Even though generation is sequential, keeping the mask ensures training/inference consistencyâ€”same attention patterns.\n",
        "\n",
        "**Common bug:** Forgetting to mask during fine-tuning leads to subtle distribution shifts.\n",
        "\n",
        "---\n",
        "\n",
        "### Attention vs Residual Dropout\n",
        "**Attention dropout:** Applied to weights: $\\text{Dropout}(\\text{softmax}(QK^T))$\n",
        "- Randomly zeros some attention connections\n",
        "\n",
        "**Residual dropout:** Applied after attention: $x + \\text{Dropout}(\\text{Attention}(x))$\n",
        "- Stochastically drops entire sublayer contributions\n",
        "\n",
        "**Modern trend:** Large models (>10B params) often skip dropout entirely. Scale provides regularization.\n",
        "\n",
        "---\n",
        "\n",
        "## Model Parallelism\n",
        "\n",
        "### Three Parallelism Types\n",
        "**Data Parallel (DP):**\n",
        "- Clone full model on N GPUs\n",
        "- Each GPU processes different batch slice\n",
        "- All-reduce gradients once per step\n",
        "- **Bottleneck:** Batch size\n",
        "\n",
        "**Pipeline Parallel (PP):**\n",
        "- Vertical split: layers 0-11 on GPU0, 12-23 on GPU1\n",
        "- Forward/backward passes bubble through pipeline\n",
        "- **Bottleneck:** Pipeline bubble time (~12% wasted)\n",
        "\n",
        "**Tensor Parallel (TP):**\n",
        "- Horizontal split: $Y = X[W_1|W_2]$ â†’ two GPUs compute $XW_1$ and $XW_2$ in parallel\n",
        "- All-gather results per layer\n",
        "- **Bottleneck:** All-gather bandwidth\n",
        "\n",
        "**In practice:** GPT-3 uses DP Ã— PP Ã— TP (3D parallelism). 8-way DP, 16-way PP, 8-way TP.\n",
        "\n",
        "---\n",
        "\n",
        "### ZeRO Stages\n",
        "**Problem:** Adam stores: params (1Ã—) + gradients (1Ã—) + momentum (1Ã—) + variance (1Ã—) = 4Ã— memory\n",
        "\n",
        "**ZeRO-1:** Partition optimizer states â†’ each GPU stores 1/N of Adam states\n",
        "**ZeRO-2:** + partition gradients â†’ further 2Ã— reduction\n",
        "**ZeRO-3:** + partition parameters â†’ each GPU has 1/N of full model\n",
        "\n",
        "**Trade-off:** ZeRO-3 = max memory efficiency but 50% more communication (must gather params before every forward/backward).\n",
        "\n",
        "**When to use:** ZeRO-2 for <10B params, ZeRO-3 for >70B params on limited GPUs.\n",
        "\n",
        "---\n",
        "\n",
        "## Context Length\n",
        "\n",
        "### Why Context Size Matters\n",
        "**Compute cost:** Attention is $O(n^2)$. Doubling context = 4Ã— memory and compute.\n",
        "\n",
        "**Position encoding limits:**\n",
        "- Sinusoidal/learned: Hard wall at training length\n",
        "- RoPE: Can extrapolate via frequency interpolation\n",
        "\n",
        "**Real numbers:** 4K context costs ~$0.0001/token. 32K costs ~$0.0006/token (6Ã— cost for 8Ã— length due to attention).\n",
        "\n",
        "---\n",
        "\n",
        "### RoPE Extrapolation (NTK-Aware, YaRN)\n",
        "**Problem:** RoPE rotates by $\\theta \\cdot m$ where $m$ is position. At 2Ã— training length, frequencies misalign.\n",
        "\n",
        "**NTK-aware scaling:** $\\theta' = \\theta \\cdot s^{d/(d-2)}$ where $s = L_{\\text{new}}/L_{\\text{train}}$\n",
        "- Stretches wavelength to match new context\n",
        "\n",
        "**YaRN (improved):** Different scaling per dimensionâ€”low frequencies scale more (capture long-range), high frequencies less (keep local precision).\n",
        "\n",
        "**Result:** LLaMA-2 4K â†’ 16K with <1% quality loss after brief fine-tuning.\n",
        "\n",
        "---\n",
        "\n",
        "## Loss & Training\n",
        "\n",
        "### Perplexity vs Cross-Entropy\n",
        "**Perplexity:** $\\text{PPL} = \\exp(\\text{loss})$\n",
        "\n",
        "**Intuition:** PPL=20 means \"model is as confused as uniform random choice from 20 tokens.\"\n",
        "\n",
        "**Useful conversions:**\n",
        "- Loss 4.0 â†’ PPL 54.6 (barely better than random on 50K vocab)\n",
        "- Loss 2.3 â†’ PPL 10 (reasonable)\n",
        "- Loss 1.5 â†’ PPL 4.5 (strong)\n",
        "\n",
        "**Why use it?** Perplexity scales linearly with \"confusion,\" loss doesn't.\n",
        "\n",
        "---\n",
        "\n",
        "### Z-Loss (PaLM Stability)\n",
        "**Problem:** Logits can explode to Â±100, causing NaN in $\\exp(100)$.\n",
        "\n",
        "**Z-loss:** $L_z = \\left(\\log \\sum_i e^{z_i}\\right)^2$\n",
        "- Penalizes large log-sum-exp (logit magnitude)\n",
        "\n",
        "**Typical weight:** $10^{-4} \\times L_z$ added to cross-entropy.\n",
        "\n",
        "**Effect:** Keeps logits centered near zero, prevents softmax saturation.\n",
        "\n",
        "---\n",
        "\n",
        "### MoE Load Balancing Loss\n",
        "**Problem:** All tokens route to expert #3, others unused (collapse).\n",
        "\n",
        "**Loss:** $L_{\\text{aux}} = \\alpha \\cdot N \\sum_{i=1}^{N} f_i \\cdot P_i$\n",
        "- $f_i$ = fraction of tokens sent to expert $i$\n",
        "- $P_i$ = average gating score for expert $i$\n",
        "\n",
        "**Intuition:** Penalize when routing frequency Ã— gating probability is high (expert overused).\n",
        "\n",
        "**Typical:** $\\alpha = 0.01$. Too high â†’ forced uniform routing hurts quality.\n",
        "\n",
        "---\n",
        "\n",
        "## Embeddings\n",
        "\n",
        "### Tied Input/Output Embeddings\n",
        "**Standard:** Input embeddings $W_e \\in \\mathbb{R}^{V \\times d}$, output projection $W_o \\in \\mathbb{R}^{d \\times V}$\n",
        "\n",
        "**Tied:** Set $W_o = W_e^T$ (share weights)\n",
        "\n",
        "**Benefits:**\n",
        "- 50% fewer params (matters when V=50K)\n",
        "- Semantic consistency: \"cat\" embedding â‰ˆ \"cat\" output direction\n",
        "- Better sample efficiency\n",
        "\n",
        "**Used in:** GPT-2, GPT-3, T5, most decoder models.\n",
        "\n",
        "---\n",
        "\n",
        "### Absolute vs Relative Positional Encoding\n",
        "**Absolute:** Position 5 gets $\\text{PE}_5$, position 10 gets $\\text{PE}_{10}$\n",
        "\n",
        "**Relative:** Attention sees *distance* $i-j$, not raw positions\n",
        "\n",
        "**Why relative wins?** \"The cat sat\" vs \"Yesterday, the cat sat\"â€”relative distance from \"sat\" to \"cat\" unchanged.\n",
        "\n",
        "**Methods:**\n",
        "- **RoPE:** Rotate Q/K by position-dependent angle\n",
        "- **ALiBi:** Add $-(i-j)$ bias to attention scores\n",
        "- **T5:** Learned bias buckets for distances\n",
        "\n",
        "---\n",
        "\n",
        "## Decoding Strategies\n",
        "\n",
        "### Greedy vs Beam vs Nucleus\n",
        "**Greedy:** Pick $\\arg\\max$ each step\n",
        "- Fast, deterministic, repetitive\n",
        "\n",
        "**Beam search (k=5):** Keep top-5 sequences at each step, expand all\n",
        "- Better quality for translation\n",
        "- 5Ã— slower, still somewhat repetitive\n",
        "\n",
        "**Nucleus (top-p=0.9):** Sample from smallest set with cumulative prob â‰¥ 0.9\n",
        "- Adaptive: sometimes 3 tokens, sometimes 50\n",
        "- Best for creative text\n",
        "\n",
        "**Rule of thumb:** Greedy for code, beam for structured tasks, nucleus for chat.\n",
        "\n",
        "---\n",
        "\n",
        "### Temperature Sampling\n",
        "**Formula:** $P(t) = \\frac{\\exp(z_t/T)}{\\sum_i \\exp(z_i/T)}$\n",
        "\n",
        "**Effect:**\n",
        "- $T=0.01$: Almost greedy (argmax)\n",
        "- $T=1$: Raw model distribution\n",
        "- $T=2$: Very random, creative but risky\n",
        "\n",
        "**Typical values:** 0.7 for chat, 0.1 for factual QA, 1.2 for creative writing.\n",
        "\n",
        "---\n",
        "\n",
        "### Repetition Penalty\n",
        "**Problem:** \"The the the the...\" loops in greedy decoding\n",
        "\n",
        "**Fix:** $P'(t) = \\frac{P(t)}{r^{\\text{count}(t)}}$ where $r=1.2$\n",
        "- Seen once: divide by 1.2\n",
        "- Seen twice: divide by 1.44\n",
        "\n",
        "**Warning:** $r>1.5$ makes output nonsensical (avoids all common words like \"the\").\n",
        "\n",
        "---\n",
        "\n",
        "## Quantization\n",
        "\n",
        "### Symmetric vs Asymmetric\n",
        "**Symmetric INT8:** Map $[-\\alpha, \\alpha] \\to [-127, 127]$\n",
        "- $q = \\text{round}(x/s)$ where $s = \\alpha/127$\n",
        "- Zero point = 0 (faster, no offset addition)\n",
        "\n",
        "**Asymmetric UINT8:** Map $[\\text{min}, \\text{max}] \\to [0, 255]$\n",
        "- $q = \\text{round}(x/s) + z$\n",
        "- Handles skewed distributions better\n",
        "\n",
        "**In practice:** Symmetric for weights (faster), asymmetric for activations (better accuracy).\n",
        "\n",
        "---\n",
        "\n",
        "### Per-Tensor vs Per-Channel\n",
        "**Per-tensor:** Single scale $s$ for entire $4096 \\times 4096$ matrix\n",
        "- Some channels under-utilize INT8 range\n",
        "\n",
        "**Per-channel:** Scale $s_i$ per output channel (row)\n",
        "- Reduces quantization error by 2-3Ã—\n",
        "- Minimal compute overhead (vectorized multiply)\n",
        "\n",
        "**Always use per-channel** for weights in LLMs.\n",
        "\n",
        "---\n",
        "\n",
        "## Attention Optimizations\n",
        "\n",
        "### PagedAttention (vLLM)\n",
        "**Problem:** KV cache is fragmented. Seq A uses 47 tokens, Seq B uses 203 â†’ memory holes.\n",
        "\n",
        "**Solution:** Store KV in fixed-size pages (e.g., 16 tokens/page). Non-contiguous.\n",
        "\n",
        "**Benefits:**\n",
        "- 2-4Ã— throughput via tighter packing\n",
        "- Enables prefix sharing (same system prompt reuses KV pages)\n",
        "\n",
        "**Analogy:** Like OS virtual memoryâ€”logical addresses map to physical blocks.\n",
        "\n",
        "---\n",
        "\n",
        "### Multi-Query Attention (MQA)\n",
        "**Standard MHA:** Each head has separate K, V matrices\n",
        "\n",
        "**MQA:** All heads share 1 K, 1 V (still separate Q per head)\n",
        "\n",
        "**Trade-off:**\n",
        "- 10-20Ã— faster KV cache loading (critical for long sequences)\n",
        "- ~2% quality loss vs MHA\n",
        "\n",
        "**Who uses it:** PaLM, Falcon, StarCoder.\n",
        "\n",
        "**Grouped-Query Attention (GQA):** Middle groundâ€”8 heads share 2 KV pairs. Used in Llama-2 70B."
      ],
      "metadata": {
        "id": "uCTYzTL7XpAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set-1"
      ],
      "metadata": {
        "id": "K1GQJTWXlrPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QnA -1\n",
        "\n",
        "## ðŸŸ¢ P0 â€“ Core ML + PyTorch + Fundamentals"
      ],
      "metadata": {
        "id": "FMDdC4prZ7aJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### How do you decide whether a model is overfitting without looking at training loss?\n",
        "\n",
        "**Key signals:**\n",
        "- **Train-val gap:** 95% train accuracy vs 70% val accuracy indicates overfitting\n",
        "- **Learning curve divergence:** Validation metric plateaus or degrades while training improves\n",
        "- **High variance across seeds:** Different random initializations give wildly different validation scores\n",
        "- **Simple example failures:** Model fails on easy held-out cases it should generalize to\n",
        "- **Production degradation:** Performance drops on recent data vs validation set (temporal overfitting)\n",
        "\n",
        "**Quick check:** Plot train vs val curves - if they diverge after epoch N, that's your overfitting point.\n",
        "\n",
        "***\n",
        "\n",
        "### When does accuracy become a misleading metric? Give 2 real examples.\n",
        "\n",
        "**Example 1: Imbalanced classes**\n",
        "- Fraud detection with 99.5% legitimate transactions\n",
        "- Model predicting \"not fraud\" always = 99.5% accuracy but 0% fraud recall\n",
        "- **Use instead:** Precision/Recall, F1, ROC-AUC, PR-AUC\n",
        "\n",
        "**Example 2: Varying misclassification costs**\n",
        "- Cancer diagnosis where false negative (missing cancer) >> false positive (false alarm)\n",
        "- 90% accuracy can hide 80% of cancer cases if cancer is 1% of dataset\n",
        "- **Use instead:** Class-specific recall, cost-weighted metrics, confusion matrix analysis\n",
        "\n",
        "***\n",
        "\n",
        "### Explain biasâ€“variance tradeoff using a real product scenario.\n",
        "\n",
        "**Scenario:** Product recommendation system\n",
        "\n",
        "**High bias (underfitting):**\n",
        "- Simple rule: \"recommend most popular items\"\n",
        "- Low train AND test performance\n",
        "- Doesn't capture user-item preference patterns\n",
        "\n",
        "**High variance (overfitting):**\n",
        "- Deep model memorizing exact user-item clicks\n",
        "- Perfect on training users, terrible on new users\n",
        "- Recommends obscure items clicked once by accident\n",
        "\n",
        "**Sweet spot:**\n",
        "- Captures genuine preference patterns without memorizing noise\n",
        "- Use: Regularization (L2, dropout), early stopping, cross-validation\n",
        "- Monitor: Train performance (bias check) + train-val gap (variance check)\n",
        "\n",
        "***\n",
        "\n",
        "### What are common sources of data leakage in industry datasets?\n",
        "\n",
        "**Temporal leakage:** Using `churn_date` to predict churn (feature created after target event)\n",
        "\n",
        "**Target leakage:** `days_since_purchase` when predicting purchase likelihood (derived from target)\n",
        "\n",
        "**Train-test contamination:** Same user/entity in both sets without proper time-based splitting\n",
        "\n",
        "**Preprocessing leakage:**\n",
        "- `fit_transform()` on full dataset, then split\n",
        "- **Correct:** `fit()` on train, `transform()` on val/test\n",
        "\n",
        "**Proxy variables:** `cancellation_reason` to predict cancellation (only exists post-event)\n",
        "\n",
        "**Data duplication:** Near-duplicates across train/test (common in web scraping)\n",
        "\n",
        "***\n",
        "\n",
        "### Why does increasing model capacity sometimes hurt generalization?\n",
        "\n",
        "**Insufficient data constraint:** Large parameter space â†’ fits noise instead of signal\n",
        "\n",
        "**Optimization difficulty:** Harder to find good minima in high-dimensional spaces (many local minima, saddle points)\n",
        "\n",
        "**Spurious correlations:** More capacity = more ability to latch onto non-transferable patterns\n",
        "\n",
        "**Double descent:** In interpolation regime (model fits training data perfectly), increased capacity hurts before it helps again\n",
        "\n",
        "**Loss of implicit regularization:** Very large models escape the \"beneficial overfitting\" zone where SGD naturally generalizes\n",
        "\n",
        "***\n",
        "\n",
        "### How do you choose a validation strategy when data is time-dependent?\n",
        "\n",
        "**Time-based splits:** Never random - prevents future leakage\n",
        "\n",
        "**Walk-forward validation:**\n",
        "- Train [tâ‚€, tâ‚] â†’ validate [tâ‚, tâ‚‚]\n",
        "- Train [tâ‚€, tâ‚‚] â†’ validate [tâ‚‚, tâ‚ƒ]\n",
        "\n",
        "**Gap period:** Leave gap between train/val to simulate production delay (e.g., model trained Monday, deployed Wednesday)\n",
        "\n",
        "**Seasonality:** Validate on similar time periods (same weekday, holidays)\n",
        "\n",
        "**Multiple windows:** Check consistency across different time periods to detect temporal instability\n",
        "\n",
        "***\n",
        "\n",
        "## Optimization & Training Stability\n",
        "\n",
        "### Why does a model's loss sometimes plateau early even though gradients are non-zero?\n",
        "\n",
        "**Learning rate too small:** Gradients point correctly but step size tiny â†’ extremely slow convergence\n",
        "\n",
        "**Saddle points:** Gradients small (not zero) but pointing in flat directions â†’ no progress\n",
        "\n",
        "**Poor conditioning:** Some parameters need 10Ã— larger updates than others, but using same LR for all\n",
        "\n",
        "**Gradient noise:** Batch gradients fluctuating too much (high variance) â†’ steps cancel out\n",
        "\n",
        "**Representational limit:** Architecture can't express the solution (e.g., linear model on XOR problem)\n",
        "\n",
        "**Mild vanishing gradients:** Early layer gradients exist but too small (~1e-8) to update effectively\n",
        "\n",
        "***\n",
        "\n",
        "### What are symptoms of a bad learning rate? How do you diagnose it?\n",
        "\n",
        "**Too high:**\n",
        "- Loss â†’ NaN or inf within first epoch\n",
        "- Loss oscillates wildly without downward trend\n",
        "- Gradient norms spike (>1000Ã—)\n",
        "\n",
        "**Too low:**\n",
        "- Loss barely moves after 10 epochs\n",
        "- Train and val loss nearly identical (not learning)\n",
        "\n",
        "**Diagnosis:**\n",
        "- **LR range test:** Train with exponentially increasing LR (1e-6 â†’ 1), plot loss vs LR\n",
        "- **Gradient norm check:** Should be stable ~0.1-10, not exploding/vanishing\n",
        "- **Parameter update ratio:** Updates should be 0.001-0.01Ã— parameter magnitude\n",
        "- **Parallel search:** Try [1e-5, 1e-4, 1e-3] on small subset quickly\n",
        "\n",
        "***\n",
        "\n",
        "### Why does batch size affect generalization?\n",
        "\n",
        "**Small batches (better generalization):**\n",
        "- High gradient noise â†’ implicit regularization\n",
        "- Explore more of loss landscape â†’ escape sharp minima\n",
        "- Sharp minima = high curvature = sensitive to perturbations â†’ poor test performance\n",
        "\n",
        "**Large batches (worse generalization):**\n",
        "- Converge to sharp minima (less noise to escape)\n",
        "- Less stochasticity = less exploration\n",
        "\n",
        "**Math:** Generalization gap âˆ (LR Ã— batch_size) / training_time\n",
        "\n",
        "**Mitigation for large batches:** Higher LR, longer warmup, label smoothing, larger model\n",
        "\n",
        "***\n",
        "\n",
        "### When would you prefer Adam over SGD â€” and vice versa?\n",
        "\n",
        "**Prefer Adam:**\n",
        "- Prototyping / quick iteration (works out-of-box)\n",
        "- Sparse gradients (NLP embeddings) - adaptive LR helps infrequent updates\n",
        "- Different parameter scales handled automatically\n",
        "- Limited hyperparameter search time\n",
        "\n",
        "**Prefer SGD + momentum:**\n",
        "- Final performance critical (often generalizes 1-2% better)\n",
        "- Well-studied architecture (ResNets, Vision Transformers)\n",
        "- Long training budget (can afford LR schedule tuning)\n",
        "- Memory constrained (Adam stores 2Ã— extra buffers: first moment + second moment)\n",
        "\n",
        "**Why SGD generalizes better:** Adam's adaptive LR can lead to sharper minima in some settings\n",
        "\n",
        "***\n",
        "\n",
        "### What happens if you train with very small batches?\n",
        "\n",
        "**High gradient variance:** Each update noisy â†’ loss curve jagged\n",
        "\n",
        "**Slower wall-clock time:** Can't utilize GPU parallelism (GPU underutilized at batch=1)\n",
        "\n",
        "**Batch statistics unstable:** BatchNorm with batch=1 or 2 â†’ running mean/var unreliable\n",
        "\n",
        "**Convergence issues:** Batch=1 may bounce around randomly, never converge\n",
        "\n",
        "**Can help generalization:** IF you can afford training time (extreme noise = regularization)\n",
        "\n",
        "**Practical floor:** Usually batch â‰¥ 16-32 for stable training\n",
        "\n",
        "***\n",
        "\n",
        "### Why does normalization (BatchNorm/LayerNorm) stabilize training?\n",
        "\n",
        "**Reduces internal covariate shift:** Layer inputs don't change distribution wildly during training\n",
        "\n",
        "**Allows higher LR:** Prevents exploding/vanishing activations â†’ gradients stay in healthy range\n",
        "\n",
        "**Smooths loss landscape:** Creates more predictable gradients (less curvature)\n",
        "\n",
        "**Implicit regularization:** BatchNorm adds noise from batch statistics â†’ acts as regularizer\n",
        "\n",
        "**Gradient flow:** Keeps activations in linear region of activation functions (prevents saturation)\n",
        "\n",
        "**Reduces initialization sensitivity:** Poor init partially corrected by normalization\n",
        "\n",
        "***\n",
        "\n",
        "### How would you identify & tackle Vanishing Gradient problem?\n",
        "\n",
        "**Identification:**\n",
        "- Gradient norms in early layers â†’ 0 (check `layer.weight.grad.norm()`)\n",
        "- Early layers don't update (weights frozen)\n",
        "- Loss plateaus but later layers still learning\n",
        "\n",
        "**Solutions:**\n",
        "- **Initialization:** He (ReLU), Xavier/Glorot (tanh/sigmoid) to maintain variance\n",
        "- **Activation:** ReLU/GELU/SwiGLU instead of sigmoid/tanh (no saturation)\n",
        "- **Architecture:** Residual connections (gradients flow through skip path)\n",
        "- **Normalization:** BatchNorm/LayerNorm stabilizes gradient magnitudes\n",
        "- **Gated RNNs:** LSTM/GRU control information flow better than vanilla RNN\n",
        "\n",
        "***\n",
        "\n",
        "### How would you identify & tackle Exploding Gradient problem?\n",
        "\n",
        "**Identification:**\n",
        "- Gradient norms spike (>100)\n",
        "- Loss â†’ NaN or oscillates wildly\n",
        "- Weights become huge suddenly\n",
        "\n",
        "**Solutions:**\n",
        "- **Gradient clipping:** `torch.nn.utils.clip_grad_norm_(params, max_norm=1.0)` rescales to threshold\n",
        "- **Weight regularization:** L2 penalty discourages large weights\n",
        "- **Lower LR:** Prevents large updates from destabilizing\n",
        "- **Batch Normalization:** Reduces dependency on initialization, manages gradient magnitudes\n",
        "\n",
        "***\n",
        "\n",
        "## PyTorch / DL Internals\n",
        "\n",
        "### What exactly does autograd track, and when does it fail?\n",
        "\n",
        "**Tracks:**\n",
        "- Computational graph of operations on `requires_grad=True` tensors\n",
        "- Backward function for each operation\n",
        "- Intermediate activations needed for chain rule\n",
        "\n",
        "**Fails on:**\n",
        "- **In-place ops:** `x += 1` corrupts graph (use `x = x + 1`)\n",
        "- **Detached tensors:** `.detach()` or `.data` breaks gradient flow\n",
        "- **Non-PyTorch ops:** NumPy operations, Python conditionals\n",
        "- **Non-differentiable ops:** `torch.argmax()`, integer indexing\n",
        "- **Graph cleared:** After `.backward()`, need `retain_graph=True` for second backward\n",
        "\n",
        "***\n",
        "\n",
        "### Why can a PyTorch model silently stop learning?\n",
        "\n",
        "**Forgot `optimizer.zero_grad()`:** Gradients accumulate â†’ nonsensical updates\n",
        "\n",
        "**Detached tensors:** `loss = loss.detach()` breaks gradient flow\n",
        "\n",
        "**Wrong loss function:** MSE for classification, or using probabilities when expecting logits\n",
        "\n",
        "**Dead neurons:** All ReLU outputs â‰¤ 0 â†’ zero gradients forever\n",
        "\n",
        "**Forgot `optimizer.step()`:** Gradients computed but weights not updated\n",
        "\n",
        "**LR decayed to ~0:** Scheduler reduced LR to 1e-10\n",
        "\n",
        "**`requires_grad=False`:** Parameters or inputs don't track gradients\n",
        "\n",
        "***\n",
        "\n",
        "### What happens if you forget to call `model.eval()` during validation?\n",
        "\n",
        "**Dropout stays active:** Randomly zeros neurons â†’ pessimistic metrics\n",
        "\n",
        "**BatchNorm uses batch statistics:** Instead of running mean/var â†’ noisy, unreliable\n",
        "\n",
        "**Consequences:**\n",
        "- Validation metrics worse than actual capability\n",
        "- Can't reproduce results (metrics change with batch composition)\n",
        "- Misleading hyperparameter decisions\n",
        "\n",
        "**Critical production bug:** Many teams shipped models without catching this\n",
        "\n",
        "***\n",
        "\n"
      ],
      "metadata": {
        "id": "0B6M3DL6Z--1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QnA - 2"
      ],
      "metadata": {
        "id": "GB0GBogPa2_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸŸ¡ P1 â€“ LLMs, Transformers & GenAI Systems\n",
        "\n",
        "### Why does self-attention scale poorly with sequence length?\n",
        "\n",
        "**Complexity:** \\(O(n^2)\\) in both memory and compute where \\(n\\) = sequence length\n",
        "\n",
        "**Math:** For each of \\(n\\) tokens, compute attention to all \\(n\\) tokens â†’ \\(n \\times n\\) matrix\n",
        "\n",
        "**Memory:** Must store \\([batch, heads, seq, seq]\\) attention matrix\n",
        "- 1000 tokens â†’ 1M pairwise scores\n",
        "- 4096 tokens â†’ 16M scores per head\n",
        "\n",
        "**Compute:** Matrix multiplication \\(QK^T\\) is \\(O(n^2 d)\\), softmax over \\(n^2\\) elements\n",
        "\n",
        "**Workarounds:**\n",
        "- Sparse attention (Longformer: local + global patterns)\n",
        "- Linear attention (approximations)\n",
        "- Flash Attention (tiling to avoid materializing full matrix)\n",
        "- Sliding window (Mistral: 4096-token window)\n",
        "\n",
        "***\n",
        "\n",
        "### Encoder vs decoder-only models â€” when do you use each?\n",
        "\n",
        "**Encoder-only (BERT):**\n",
        "- **Attention:** Bidirectional (sees full context)\n",
        "- **Use cases:** Classification, NER, embeddings, Q&A extraction (when you have full input)\n",
        "- **Can't:** Generate text autoregressively\n",
        "\n",
        "**Decoder-only (GPT):**\n",
        "- **Attention:** Causal (only left context)\n",
        "- **Use cases:** Text generation, chat, completion, code generation\n",
        "- **Why dominant:** Simpler architecture, scales better, unified pretraining = generation\n",
        "\n",
        "**Encoder-decoder (T5):**\n",
        "- **Structure:** Encoder (bidirectional) + decoder (causal)\n",
        "- **Use cases:** Translation, summarization (different input/output)\n",
        "- **Less common now:** Decoder-only can do these via prompting\n",
        "\n",
        "***\n",
        "\n",
        "### Why do LLMs hallucinate even when prompted carefully?\n",
        "\n",
        "**Training objective:** Next-token prediction â‰  factual accuracy (optimizes plausibility, not truth)\n",
        "\n",
        "**No grounding:** Pattern matching from training data, no access to external facts\n",
        "\n",
        "**Overconfidence:** High probability doesn't mean factually correct (just statistically likely continuation)\n",
        "\n",
        "**Conflicting training data:** Seen different \"facts\" about same topic\n",
        "\n",
        "**Can't say \"I don't know\":** Always generates something (sampling from distribution)\n",
        "\n",
        "**Context limitations:** Long conversations lose track of what was stated vs inferred\n",
        "\n",
        "**Prompt misinterpretation:** Model's \"understanding\" differs from user intent\n",
        "\n",
        "***\n",
        "\n",
        "### What changes between pretraining and fine-tuning?\n",
        "\n",
        "**Pretraining:**\n",
        "- **Objective:** Next-token prediction on massive unlabeled corpus\n",
        "- **Data:** Trillions of tokens (web crawl, books, code)\n",
        "- **Goal:** Learn language patterns, world knowledge, reasoning\n",
        "- **Cost:** $millions, months of training\n",
        "- **LR:** 1e-4 to 6e-4 typical\n",
        "\n",
        "**Fine-tuning:**\n",
        "- **Objective:** Task-specific (instruction following, Q&A, summarization)\n",
        "- **Data:** Thousands to millions of curated examples\n",
        "- **Goal:** Align to specific behavior/format\n",
        "- **Cost:** Much cheaper (single GPU, hours/days)\n",
        "- **LR:** 1e-5 to 1e-6 (much smaller to avoid catastrophic forgetting)\n",
        "\n",
        "***\n",
        "\n",
        "### Why does instruction tuning improve usability?\n",
        "\n",
        "**Alignment:** Model learns to follow user intent, not just predict text\n",
        "\n",
        "**Format consistency:** Responds in expected structure (markdown, code blocks, numbered lists)\n",
        "\n",
        "**Reduced prompt engineering:** Works with natural prompts instead of careful formatting\n",
        "\n",
        "**Safety:** Can refuse harmful requests, stay on topic\n",
        "\n",
        "**Zero-shot improvement:** Better at new tasks without examples\n",
        "\n",
        "**Conversational behavior:** Learns helpful assistant patterns (clarifying questions, admitting uncertainty)\n",
        "\n",
        "***\n",
        "\n",
        "### What is the role of positional encodings?\n",
        "\n",
        "**Core problem:** Attention is permutation-invariant â†’ \"cat sat mat\" = \"mat sat cat\" without positions\n",
        "\n",
        "**Purpose:** Inject token position information so model knows word order\n",
        "\n",
        "**Types:**\n",
        "\n",
        "**Absolute:**\n",
        "- Sinusoidal: $(PE_{(pos, 2i)} = \\sin(pos/10000^{2i/d}))$\n",
        "- Learned embeddings (GPT-2)\n",
        "- **Issue:** Can't extrapolate beyond training length\n",
        "\n",
        "**Relative (modern):**\n",
        "- **RoPE:** Rotates Q/K vectors by position-dependent angle (LLaMA, GPT-NeoX)\n",
        "- **ALiBi:** Adds bias $(-m|i-j|)$ to attention scores (BLOOM, MPT)\n",
        "- **Benefit:** Generalizes to longer sequences\n",
        "\n",
        "**Why it matters:** Enables syntax understanding, temporal reasoning, dependency parsing\n",
        "\n",
        "***\n",
        "\n",
        "## Inference & Behavior\n",
        "\n",
        "### Why is LLM inference slow compared to training per token?\n",
        "\n",
        "**Training:** Batched - process 1000s of sequences in parallel â†’ high GPU utilization\n",
        "\n",
        "**Inference:** Sequential autoregressive - must generate one token at a time\n",
        "\n",
        "**Memory-bound:** Constantly loading weights from VRAM (not compute-bound like training)\n",
        "\n",
        "**KV-cache management:** Store/retrieve cached keys/values for all previous tokens\n",
        "\n",
        "**Low batch size:** Often serving 1 user at a time (can't parallelize across batch like training)\n",
        "\n",
        "**Per-token cost:** Each token requires full forward pass through all layers\n",
        "\n",
        "**Key difference:** Training processes many tokens simultaneously; inference generates one token per step\n",
        "\n",
        "***\n",
        "\n",
        "### What happens during prefill vs decoding?\n",
        "\n",
        "**Prefill (prompt processing):**\n",
        "- Process all prompt tokens **in parallel**\n",
        "- Compute KV cache for entire prompt at once\n",
        "- Single forward pass for N prompt tokens\n",
        "- Fast - fully utilizes GPU (batch matrix ops)\n",
        "\n",
        "**Decoding (generation):**\n",
        "- Generate tokens **sequentially** one at a time\n",
        "- Each step: attend to all previous tokens (growing KV cache)\n",
        "- N tokens = N separate forward passes\n",
        "- Slow - can't parallelize (autoregressive dependency)\n",
        "\n",
        "**Optimization focus:**\n",
        "- Prefill â†’ maximize throughput\n",
        "- Decoding â†’ minimize per-token latency\n",
        "\n",
        "***\n",
        "\n",
        "### Why does temperature affect factual accuracy?\n",
        "\n",
        "**Temperature = 0 (greedy):** Always pick $(\\arg\\max P(token))$\n",
        "- Deterministic, most factual\n",
        "- Can get stuck in loops\n",
        "\n",
        "**Low temperature (0.1-0.5):** Slightly randomized\n",
        "- Reduces repetition\n",
        "- Still mostly factual\n",
        "\n",
        "**High temperature (0.8-1.5):** Flatter distribution\n",
        "\n",
        "**Math:** $P(token) = \\frac{e^{logit/T}}{\\sum_i e^{logit_i/T}}$\n",
        "\n",
        "**Effect of high T:**\n",
        "- Samples from lower-probability (less reliable) tokens\n",
        "- More creative but less accurate\n",
        "- Can drift into hallucinations\n",
        "\n",
        "**Use cases:**\n",
        "- Factual Q&A â†’ T=0.1-0.3\n",
        "- Creative writing â†’ T=0.7-1.0\n",
        "- Code generation â†’ T=0.1-0.2\n",
        "\n",
        "***\n",
        "\n",
        "### When does increasing context length degrade performance?\n",
        "\n",
        "**Lost in the middle:** Model struggles to use information from middle of long contexts (proven empirically)\n",
        "\n",
        "**Attention dilution:** Attention spread thin across many tokens â†’ weaker signal per token\n",
        "\n",
        "**Position extrapolation:** Trained on 2K context, using 8K may break positional encodings\n",
        "\n",
        "**Noise accumulation:** More irrelevant text to filter through\n",
        "\n",
        "**Practical observation:** Performance often peaks at 4K-8K tokens, then degrades beyond training length\n",
        "\n",
        "**When it helps:** Specific info retrieval, long documents where answer is definitely in context\n",
        "\n",
        "***\n",
        "\n",
        "### Why do LLMs struggle with counting or exact arithmetic?\n",
        "\n",
        "**Tokenization inconsistency:** `1234` â†’ `[\"12\", \"34\"]` or `[\"1\", \"234\"]` (varies by tokenizer)\n",
        "\n",
        "**No symbolic reasoning:** Pattern matching, not actual computation\n",
        "\n",
        "**Position tracking:** Counting requires precise positional awareness across many tokens\n",
        "\n",
        "**Training bias:** Seen more small numbers than large (data distribution skew)\n",
        "\n",
        "**Approximation by design:** Learned to approximate, not compute exactly\n",
        "\n",
        "**No scratchpad:** Can't do multi-step calculation systematically (unless using chain-of-thought)\n",
        "\n",
        "**Workarounds:** Chain-of-thought prompting, tool use (calculator APIs), code generation\n",
        "\n",
        "***\n",
        "\n",
        "## Prompting vs Finetuning\n",
        "\n",
        "### When is prompt engineering insufficient?\n",
        "\n",
        "**Consistent format required:** Need exact JSON structure every time (prompts can be brittle)\n",
        "\n",
        "**Domain-specific knowledge:** Technical jargon not in pretraining data\n",
        "\n",
        "**Latency constraints:** Long prompts (500+ tokens) slow inference\n",
        "\n",
        "**Cost at scale:** Prompts expensive when serving millions of requests (pay per token)\n",
        "\n",
        "**Reliability:** Can't afford prompt brittleness in production\n",
        "\n",
        "**Complex multi-step reasoning:** Need learned shortcuts, not verbose prompting\n",
        "\n",
        "**Style/tone:** Consistent brand voice hard to maintain via prompting alone\n",
        "\n",
        "***\n",
        "\n",
        "### How do you decide between: Prompting, Fine-tuning, LoRA/adapters?\n",
        "\n",
        "**Prompting:**\n",
        "- **When:** Quick prototyping, low volume (<1K requests/day), task known to base model\n",
        "- **Pros:** No training, flexible, fast iteration\n",
        "- **Cons:** Token cost, brittleness, less control\n",
        "\n",
        "**LoRA (sweet spot for most):**\n",
        "- **When:** Need customization, limited budget/data\n",
        "- **Pros:** Train 0.1-1% of parameters â†’ 100Ã— faster/cheaper than full fine-tuning\n",
        "- **Pros:** Multiple adapters on one base model, lower forgetting risk\n",
        "- **Cons:** Slightly lower performance than full fine-tuning\n",
        "\n",
        "**Full Fine-tuning:**\n",
        "- **When:** High volume, maximum performance needed, have compute budget\n",
        "- **Pros:** Best performance, fully customized\n",
        "- **Cons:** Expensive, risk catastrophic forgetting, separate model to maintain\n",
        "\n",
        "**Decision tree:**\n",
        "1. Try prompting first (cheapest validation)\n",
        "2. If prompt >300 tokens or unreliable â†’ LoRA\n",
        "3. If performance critical and have budget â†’ full fine-tune\n",
        "\n",
        "***\n",
        "\n",
        "### Why can fine-tuning reduce generalization?\n",
        "\n",
        "**Catastrophic forgetting:** Overwrites general knowledge with task-specific patterns\n",
        "\n",
        "**Narrow distribution:** Training on medical Q&A â†’ worse at coding, creative writing\n",
        "\n",
        "**Format overfitting:** Learns to match training format, not understand deeply\n",
        "\n",
        "**Example:** Fine-tune on customer support â†’ loses capability on math, translation, code\n",
        "\n",
        "**Mitigation:**\n",
        "- Use LoRA (freezes 99% of model)\n",
        "- Mix general data (20-30%) with task data during fine-tuning\n",
        "- Lower LR (1e-6 instead of 1e-4)\n",
        "- Early stopping\n",
        "\n",
        "***\n",
        "\n",
        "### What are risks of overfitting during instruction tuning?\n",
        "\n",
        "**Memorizing examples:** Repeats exact training responses word-for-word\n",
        "\n",
        "**Format lock-in:** Only works with training-style prompts, fails on variations\n",
        "\n",
        "**Reward hacking:** Learns to game evaluation metrics instead of actual quality\n",
        "\n",
        "**Sycophancy:** Agrees with user even when wrong (if trained on always-agreeing data)\n",
        "\n",
        "**Loss of breadth:** Forgets how to do tasks not in fine-tuning data\n",
        "\n",
        "**Signs:**\n",
        "- Perfect train metrics, poor eval\n",
        "- Responses sound \"canned\"\n",
        "- Fails on rephrased questions\n",
        "\n",
        "***\n",
        "\n",
        "## Evaluation & Reliability\n",
        "\n",
        "### Why is BLEU/ROUGE insufficient for LLM evaluation?\n",
        "\n",
        "**Surface-level matching:** Only measures n-gram overlap, ignores semantics\n",
        "\n",
        "**Example:** \"The car is red\" vs \"The vehicle is crimson\" = low BLEU but same meaning\n",
        "\n",
        "**Multiple correct answers:** \"Yes, definitely\" vs \"Absolutely\" = low score but both correct\n",
        "\n",
        "**Can't measure:**\n",
        "- Factuality (fluent nonsense scores high)\n",
        "- Reasoning quality\n",
        "- Helpfulness\n",
        "- Safety\n",
        "\n",
        "**Better alternatives:**\n",
        "- LLM-as-judge (GPT-4 scoring responses)\n",
        "- Human evaluation\n",
        "- Task-specific metrics (exact match for extraction, accuracy for Q&A)\n",
        "- Embedding similarity (semantic, not n-gram)\n",
        "\n",
        "***\n",
        "\n",
        "### How do you detect hallucinations systematically?\n",
        "\n",
        "**Factual consistency:**\n",
        "- Cross-reference with knowledge base\n",
        "- Check citations exist and support claims\n",
        "- Compare with search results\n",
        "\n",
        "**Self-consistency:**\n",
        "- Ask same question 3 different ways\n",
        "- Check if answers align\n",
        "\n",
        "**Confidence scoring:**\n",
        "- Monitor token probabilities (low confidence often = hallucination)\n",
        "- Aggregate uncertainty across tokens\n",
        "\n",
        "**Adversarial probing:**\n",
        "- Ask about non-existent entities (e.g., \"Tell me about the 2025 Mars Olympics\")\n",
        "- Nonsense questions to test if model refuses\n",
        "\n",
        "**Retrieval validation (RAG):**\n",
        "- Check if answer grounded in retrieved docs\n",
        "- Attribution matching\n",
        "\n",
        "**Automated tools:**\n",
        "- Fact-checking APIs\n",
        "- NER verification\n",
        "- LLM-based hallucination detectors\n",
        "\n",
        "***\n",
        "\n",
        "### What does \"LLM drift\" mean?\n",
        "\n",
        "**Definition:** Model behavior changes over time without explicit updates\n",
        "\n",
        "**Causes:**\n",
        "- Provider updates model weights (fine-tuning, RLHF)\n",
        "- Input distribution shifts\n",
        "- Context/prompt caching issues\n",
        "- API version changes\n",
        "\n",
        "**Symptoms:**\n",
        "- Previously working prompts fail\n",
        "- Output format changes\n",
        "- Different responses to identical inputs\n",
        "- Performance degradation on benchmarks\n",
        "\n",
        "**Mitigation:**\n",
        "- Pin specific model versions (`gpt-4-0613` not `gpt-4`)\n",
        "- Regular evaluation on held-out test set\n",
        "- Monitor output distribution (log predictions)\n",
        "- A/B test before deploying new versions\n",
        "\n",
        "***\n",
        "\n",
        "### How do you evaluate changes in prompts safely?\n",
        "\n",
        "**Held-out test set:** Never tune prompts on eval data (contamination)\n",
        "\n",
        "**Diverse examples:** Cover edge cases, different input types, failure modes\n",
        "\n",
        "**Regression testing:** Ensure new prompt doesn't break existing cases\n",
        "\n",
        "**A/B testing:** Run both prompts on 5% â†’ 20% â†’ 50% â†’ 100% traffic\n",
        "\n",
        "**Multiple evaluators:** Human raters + automated metrics (avoid single point of failure)\n",
        "\n",
        "**Version control:** Git-track all prompts with performance metadata\n",
        "\n",
        "**Key metrics:**\n",
        "- Task accuracy/F1\n",
        "- Human preference score\n",
        "- Latency (longer prompts = slower)\n",
        "- Failure rate (% of malformed outputs)\n",
        "\n",
        "***\n",
        "\n",
        "### Why do offline metrics often disagree with user feedback?\n",
        "\n",
        "**Proxy vs real goal:** ```Optimizing BLEU â‰  user satisfaction```\n",
        "\n",
        "**Missing context:** Users have intent/background not in test set\n",
        "\n",
        "**Subjective quality:** \"Helpfulness\" hard to metric (tone, conciseness preferences vary)\n",
        "\n",
        "**User expectations evolve:** What's \"good\" changes over time, varies by user segment\n",
        "\n",
        "**Latency matters:** Slow but perfect < fast but good enough (offline metrics ignore speed)\n",
        "\n",
        "**Business metrics differ:** User retention/engagement â‰  model accuracy\n",
        "\n",
        "**Solution:** Combine offline metrics + online A/B tests + user feedback loops\n",
        "\n",
        "***\n"
      ],
      "metadata": {
        "id": "ALKVDo5Ea6H1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QnA -3\n",
        "## ðŸŸ  P2 â€“ Applied NLP / DL Depth"
      ],
      "metadata": {
        "id": "E66jYlLsl2yT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "### Why do embeddings capture semantic similarity?\n",
        "\n",
        "**Distributional hypothesis:** \"Words in similar contexts have similar meanings\"\n",
        "\n",
        "**Training objective:** Predict context (Word2Vec) or masked tokens (BERT)\n",
        "- Minimize loss â†’ similar usage patterns â†’ similar vectors\n",
        "\n",
        "**Geometry emerges:** Distance in embedding space = semantic distance\n",
        "\n",
        "**Math:** Words that co-occur frequently get gradients pulling them closer\n",
        "\n",
        "**Example:** \"king\" and \"queen\" appear in similar contexts (royalty, ruler) â†’ close embeddings\n",
        "\n",
        "**Compositional:** Vector arithmetic works: \\(\\vec{king} - \\vec{man} + \\vec{woman} \\approx \\vec{queen}\\)\n",
        "\n",
        "***\n",
        "\n",
        "### When do cosine similarity and dot product behave differently?\n",
        "\n",
        "**Cosine similarity:**\n",
        "\\[\\text{cos}(\\theta) = \\frac{A \\cdot B}{||A|| \\cdot ||B||}\\]\n",
        "- Measures angle, ignores magnitude\n",
        "- Range: [-1, 1]\n",
        "- **Use when:** Vector magnitude meaningless (text embeddings)\n",
        "\n",
        "**Dot product:**\n",
        "\\[A \\cdot B = ||A|| \\cdot ||B|| \\cdot \\text{cos}(\\theta)\\]\n",
        "- Measures angle AND magnitude\n",
        "- Range: unbounded\n",
        "- **Use when:** Magnitude matters (attention scores in transformers)\n",
        "\n",
        "**Key difference:**\n",
        "- Normalized embeddings: cosine = dot product\n",
        "- Unnormalized: dot product prefers longer vectors\n",
        "\n",
        "**Practice:** Most retrieval systems normalize embeddings, then use dot product (faster than cosine)\n",
        "\n",
        "***\n",
        "\n",
        "### Why does embedding quality degrade for long documents?\n",
        "\n",
        "**Averaging problem:** Mean pooling of 1000 tokens dilutes specific information\n",
        "\n",
        "**Fixed dimensionality bottleneck:** 768-dim vector must compress thousands of tokens\n",
        "\n",
        "**Attention dilution:** Model can't focus equally on all parts\n",
        "\n",
        "**Context limits:** Exceeds training context length (e.g., BERT trained on 512 tokens)\n",
        "\n",
        "**Information loss:** Compressing too much into fixed size\n",
        "\n",
        "**Solutions:**\n",
        "- Chunk documents, embed separately\n",
        "- Weighted pooling (attention-based aggregation)\n",
        "- Hierarchical embeddings (chunk â†’ document)\n",
        "- Long-context models (Longformer)\n",
        "\n",
        "***\n",
        "\n",
        "### What causes embedding collapse?\n",
        "\n",
        "**Definition:** All embeddings converge to same/very similar vectors\n",
        "\n",
        "**Causes:**\n",
        "- Poor loss function (doesn't enforce separation)\n",
        "- Insufficient negative sampling (no contrast)\n",
        "- Too strong normalization pushing all to unit sphere center\n",
        "- Model bottleneck too tight (information loss)\n",
        "- Vanishing gradients\n",
        "\n",
        "**Symptoms:**\n",
        "- All cosine similarities near 1.0\n",
        "- Retrieval returns random results\n",
        "- Loss decreases but embeddings don't separate\n",
        "\n",
        "**Prevention:**\n",
        "- Contrastive learning with hard negatives\n",
        "- Monitor embedding variance during training (should be >0.1)\n",
        "- Use triplet/InfoNCE loss (enforces separation)\n",
        "\n",
        "***\n",
        "\n",
        "### Why does tokenization matter for performance?\n",
        "\n",
        "**Vocabulary efficiency:** BPE/WordPiece balance vocab size vs coverage\n",
        "\n",
        "**Sequence length:** Poor tokenization â†’ longer sequences â†’ slower, more memory\n",
        "\n",
        "**Semantic preservation:** \"unhappy\" as 1 token vs [\"un\", \"happy\"] affects meaning capture\n",
        "\n",
        "**OOV handling:** Character-level handles any text but too long; word-level fails on unseen words\n",
        "\n",
        "**Cross-lingual:** Good tokenization shares subwords across languages\n",
        "\n",
        "**Numbers/code:** Different tokenizers split differently\n",
        "- Good: `\"ChatGPT\"` â†’ `[\"Chat\", \"GPT\"]`\n",
        "- Bad: `\"ChatGPT\"` â†’ `[\"Chat\", \"G\", \"PT\"]`\n",
        "\n",
        "**Impact:** Can cause 10-20% performance difference on domain-specific tasks\n",
        "\n",
        "***\n",
        "\n",
        "### How can tokenization introduce bias?\n",
        "\n",
        "**Language bias:**\n",
        "- English word = 1 token\n",
        "- Same Chinese word = 3-4 tokens\n",
        "- **Impact:** Chinese users pay 3Ã— more, slower inference, less context\n",
        "\n",
        "**Name bias:**\n",
        "- \"John Smith\" = 2 tokens\n",
        "- \"Sundarajan Pichai\" = 6 tokens\n",
        "- **Impact:** Non-Western names consume more context window\n",
        "\n",
        "**Domain bias:**\n",
        "- \"FinTech\" â†’ `[\"Fin\", \"Tech\"]` vs `[\"FinTech\"]`\n",
        "- Technical terms split badly if not in training data\n",
        "\n",
        "**Case sensitivity:**\n",
        "- \"Hello\" and \"hello\" as separate tokens (wasteful)\n",
        "\n",
        "**Consequences:** Different user groups experience different model quality, cost, latency\n",
        "\n",
        "***\n",
        "\n",
        "### Why does truncation hurt certain NLP tasks more than others?\n",
        "\n",
        "**Hurts more:**\n",
        "- **Long-form Q&A:** Answer often at end of document (truncating = losing answer)\n",
        "- **Summarization:** Need full document context\n",
        "- **Legal/medical:** Critical info anywhere, can't afford to lose\n",
        "- **Multi-turn dialogue:** Early conversation context needed for coherence\n",
        "\n",
        "**Hurts less:**\n",
        "- **Classification:** Often first sentences sufficient (sentiment, topic)\n",
        "- **NER:** Local context usually enough\n",
        "- **Short Q&A:** Context already fits\n",
        "\n",
        "**Solutions:**\n",
        "- Sliding window with overlap\n",
        "- Chunk and aggregate predictions\n",
        "- Hierarchical processing (summarize chunks, then aggregate)\n",
        "\n",
        "***\n",
        "\n"
      ],
      "metadata": {
        "id": "UAHFKw3fl3G4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QnA -4\n",
        "## ðŸŸ P3 - Fine-Tuning,Loss Functions & Training"
      ],
      "metadata": {
        "id": "ws1_WxJra8WY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-Tuning & Data\n",
        "\n",
        "### Why does fine-tuning sometimes reduce zero-shot performance?\n",
        "\n",
        "**Catastrophic forgetting:** Weights updated to specialize â†’ lose general capabilities\n",
        "\n",
        "**Distribution shift:** Training on narrow domain shifts learned distribution away from general knowledge\n",
        "\n",
        "**Format conditioning:** Model expects fine-tuning format, fails on raw/diverse prompts\n",
        "\n",
        "**Task interference:** New task patterns overwrite old knowledge in shared parameter space\n",
        "\n",
        "**Example:** Fine-tune on customer support â†’ worse at creative writing, math, coding, translation\n",
        "\n",
        "**Mitigation:**\n",
        "- **LoRA:** Freezes 99% of parameters, only trains adapters\n",
        "- Mix 20-30% general data during fine-tuning\n",
        "- Lower LR (1e-6 vs 1e-4)\n",
        "- Fewer epochs (1-3 instead of 10)\n",
        "\n",
        "***\n",
        "\n",
        "### What data issues cause catastrophic forgetting?\n",
        "\n",
        "**Too specialized:** Only task-specific examples, no general data to anchor knowledge\n",
        "\n",
        "**Too few examples:** Overfits quickly to narrow distribution (<100 examples dangerous)\n",
        "\n",
        "**High learning rate:** Aggressively overwrites pretrained weights (use 10-100Ã— smaller LR than pretraining)\n",
        "\n",
        "**Too many epochs:** First epoch helps, epoch 10 causes forgetting\n",
        "\n",
        "**Imbalanced data:** Model forgets underrepresented capabilities\n",
        "\n",
        "**Format mismatch:** Very different format from pretraining (e.g., only JSON outputs)\n",
        "\n",
        "**No regularization:** Full fine-tuning without constraints\n",
        "\n",
        "***\n",
        "\n",
        "### How much data is \"enough\" for fine-tuning?\n",
        "\n",
        "**Rule of thumb:**\n",
        "\n",
        "**Instruction tuning (general):** 1K-10K diverse examples\n",
        "\n",
        "**Domain adaptation:** 10K-100K examples\n",
        "\n",
        "**Specific narrow task:** 100-1K examples\n",
        "\n",
        "**Quality > quantity:**\n",
        "- 1K high-quality, diverse examples > 10K repetitive ones\n",
        "- Need coverage of edge cases, variations, failure modes\n",
        "\n",
        "**Signs of insufficient data:**\n",
        "- Memorizing training examples verbatim\n",
        "- High variance between runs (different seeds = different models)\n",
        "- Train accuracy 95%, val accuracy 60%\n",
        "\n",
        "**Modern reality with LoRA:**\n",
        "- Can fine-tune effectively with 500-1K examples\n",
        "- Less data needed than full fine-tuning (less overfitting risk)\n",
        "\n",
        "***\n",
        "\n",
        "### Why does class imbalance affect neural models differently than tree models?\n",
        "\n",
        "**Tree models (Random Forest, XGBoost):**\n",
        "- Each split can find patterns in minority class\n",
        "- Ensemble averaging helps\n",
        "- Naturally handle imbalance via feature splits\n",
        "\n",
        "**Neural networks:**\n",
        "- **Gradient-based learning biased toward majority class**\n",
        "- Loss dominated by majority: \\(L = \\frac{1}{N}\\sum L_i\\) where 99% are majority\n",
        "- Updates mostly help majority (minority barely affects gradients)\n",
        "- Can collapse to always predicting majority (99% accuracy on 99:1 imbalance!)\n",
        "\n",
        "**Solutions for NNs:**\n",
        "- **Class weights:** \\(L = \\sum w_i L_i\\) where \\(w_{minority} = 10 \\times w_{majority}\\)\n",
        "- Focal loss: \\(FL = -(1-p_t)^\\gamma \\log(p_t)\\) (focuses on hard examples)\n",
        "- Oversample minority or undersample majority\n",
        "- Two-stage training (balance first, then full data)\n",
        "\n",
        "***\n",
        "\n",
        "## Loss Functions & Training\n",
        "\n",
        "### Why does cross-entropy work well for language modeling?\n",
        "\n",
        "**Probabilistic interpretation:** Naturally models \\(P(\\text{next token} | \\text{context})\\)\n",
        "\n",
        "**Penalizes confidence on wrong predictions:** Not just binary right/wrong\n",
        "- Predicting \"cat\" with 99% when answer is \"dog\" = huge loss\n",
        "- Predicting \"cat\" with 51% when answer is \"dog\" = smaller loss\n",
        "\n",
        "**Formula:** \\(L = -\\sum_{i} y_i \\log(\\hat{y}_i)\\) where \\(y\\) is one-hot, \\(\\hat{y}\\) is predicted distribution\n",
        "\n",
        "**Smooth gradients:** Provides learning signal across all probabilities (not just argmax)\n",
        "\n",
        "**Information-theoretic:** Minimizes KL divergence between predicted and true distributions\n",
        "\n",
        "**Well-calibrated:** Output probabilities interpretable as confidence\n",
        "\n",
        "**Natural pairing with softmax:**\n",
        "\\[\\frac{\\partial L}{\\partial z_i} = \\hat{y}_i - y_i\\]\n",
        "Clean gradient (no vanishing/exploding)\n",
        "\n",
        "***\n",
        "\n",
        "### When would you modify the loss instead of the model?\n",
        "\n",
        "**Class imbalance:** Weighted cross-entropy simpler than changing architecture\n",
        "\n",
        "**Hard negative mining:** Focal loss focuses on difficult examples without architectural change\n",
        "\n",
        "**Multi-task learning:** Combined loss \\(L = \\alpha L_1 + \\beta L_2\\) easier than multi-head architecture\n",
        "\n",
        "**Regularization:** Add L1/L2 penalty: \\(L_{total} = L_{task} + \\lambda ||W||^2\\)\n",
        "\n",
        "**Robustness:** Label smoothing for noisy labels\n",
        "\n",
        "**Calibration:** Temperature scaling in loss for better probability estimates\n",
        "\n",
        "**Curriculum learning:** Gradually increase loss weight on hard examples\n",
        "\n",
        "**Advantage:** Easier to implement, tune, debug than architectural changes\n",
        "\n",
        "***\n",
        "\n",
        "### Why does label noise hurt deep models more?\n",
        "\n",
        "**High capacity:** Neural networks can memorize noise (10M parameters can fit random labels)\n",
        "\n",
        "**Confidence:** Become overconfident on mislabeled examples (softmax â†’ 0.99 on wrong label)\n",
        "\n",
        "**Gradient propagation:** Wrong labels send wrong signals throughout entire network\n",
        "\n",
        "**Many epochs:** More exposure to noisy labels â†’ more memorization\n",
        "\n",
        "**Tree models more robust:**\n",
        "- Local decisions less affected by single mislabel\n",
        "- Ensemble averages out noise\n",
        "- Less prone to memorizing individual examples\n",
        "\n",
        "**Solutions:**\n",
        "- Label smoothing: \\(y' = (1-\\epsilon)y + \\epsilon/K\\) where \\(\\epsilon=0.1\\)\n",
        "- Confident learning (identify and remove/reweight noisy labels)\n",
        "- MixUp augmentation\n",
        "- Early stopping (before memorization phase)\n",
        "\n",
        "***\n"
      ],
      "metadata": {
        "id": "o7uAVxZta-ji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## QnA -5\n",
        "## ðŸ”µ P4 â€“ RAG, Optimization & Practical System Design"
      ],
      "metadata": {
        "id": "nTEzp78imUPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Why does naive top-k retrieval often fail?\n",
        "\n",
        "**Semantic vs keyword mismatch:** Query \"How to fix error?\" vs document \"Troubleshooting guide\" (different words, same meaning)\n",
        "\n",
        "**Query too vague:** \"Tell me about it\" â†’ what's \"it\"?\n",
        "\n",
        "**Query too specific:** \"Python 3.11.2 bug fix\" retrieves narrow results, misses general Python debugging docs\n",
        "\n",
        "**Chunk boundaries:** Answer split across chunks (chunk 1 has question, chunk 2 has answer)\n",
        "\n",
        "**Poor chunking:**\n",
        "- Too small (100 tokens) â†’ loses context\n",
        "- Too large (1000 tokens) â†’ retrieves noise with signal\n",
        "\n",
        "**Embedding quality:** Generic embeddings (all-MiniLM) don't capture domain jargon\n",
        "\n",
        "**No query understanding:** Treats \"best restaurants\" same as \"worst restaurants\" (negation lost)\n",
        "\n",
        "**Recency bias:** Vector DB might not have latest docs\n",
        "\n",
        "***\n",
        "\n",
        "### How does chunk size affect retrieval quality?\n",
        "\n",
        "**Too small (100-200 tokens):**\n",
        "- **Pros:** Precise retrieval, more granular\n",
        "- **Cons:**\n",
        "  - Loses context (answer spans multiple chunks)\n",
        "  - Need to retrieve 10+ chunks to get full context\n",
        "  - Higher noise (many irrelevant chunks)\n",
        "\n",
        "**Too large (1000+ tokens):**\n",
        "- **Pros:** Better context preservation\n",
        "- **Cons:**\n",
        "  - Less precise (retrieves irrelevant info with relevant)\n",
        "  - Fewer total chunks â†’ worse recall\n",
        "  - Embeddings less focused (averaging over too much)\n",
        "\n",
        "**Sweet spot:** 300-500 tokens with 50-100 token overlap\n",
        "\n",
        "**Advanced:**\n",
        "- Hierarchical chunking (summaries + detail chunks)\n",
        "- Semantic chunking (split by topic change, not fixed size)\n",
        "- Query-dependent chunking\n",
        "\n",
        "***\n",
        "\n",
        "### Why does retrieval sometimes reduce answer quality?\n",
        "\n",
        "**Retrieves wrong information:** Misleads model worse than no retrieval\n",
        "\n",
        "**Contradicts model knowledge:** Retrieved text says \"Paris is in Germany\" â†’ creates confusion\n",
        "\n",
        "**Noise injection:** 5 retrieved chunks, only 1 relevant â†’ other 4 distract\n",
        "\n",
        "**Context stuffing:** Too many chunks exceed context limit â†’ information lost or truncated\n",
        "\n",
        "**Hallucination anchoring:** Model forced to use bad retrieval, produces worse hallucination than generating from knowledge\n",
        "\n",
        "**Query-document mismatch:** Semantic search finds superficially similar but contextually wrong docs\n",
        "\n",
        "**When to skip retrieval:**\n",
        "- Model is confident (high token probability)\n",
        "- Query is general knowledge (\"What is gravity?\")\n",
        "- Retrieval confidence score <0.7\n",
        "\n",
        "***\n",
        "\n",
        "### When does reranking help most?\n",
        "\n",
        "**Initial retrieval is noisy:** Top-10 has 7 false positives â†’ reranking filters them\n",
        "\n",
        "**Need precision over recall:** Better to have 3 great results than 10 mixed\n",
        "\n",
        "**Complex matching:** Beyond simple semantic similarity\n",
        "- Negation (\"not Windows\")\n",
        "- Temporal constraints (\"after 2023\")\n",
        "- Specific requirements (\"Python 3.11+ only\")\n",
        "\n",
        "**Cost-performance tradeoff:**\n",
        "- Cheap embedding search on 1000 docs â†’ top-50\n",
        "- Expensive cross-encoder reranking on top-50 â†’ top-5\n",
        "\n",
        "**Reranking models:**\n",
        "- Cross-encoders (query + document encoded together)\n",
        "- LLM-based scoring (GPT-4 rates relevance)\n",
        "- Hybrid (keyword + semantic)\n",
        "\n",
        "**When it doesn't help:**\n",
        "- Initial retrieval already high quality (>0.9 relevance)\n",
        "- Latency critical (reranking adds 100-300ms)\n",
        "\n",
        "***\n",
        "\n",
        "### How do you debug a bad RAG response?\n",
        "\n",
        "**Check retrieval quality:**\n",
        "- Are correct chunks retrieved? (manually inspect top-5)\n",
        "- Similarity scores? (should be >0.7 for good retrieval)\n",
        "- Try different queries (rephrase question)\n",
        "\n",
        "**Check chunks:**\n",
        "- Is answer actually in retrieved chunks? (read them!)\n",
        "- Properly formatted? (no encoding issues)\n",
        "- Context window not exceeded? (count tokens)\n",
        "\n",
        "**Check query:**\n",
        "- Well-formed? (\"Fix bug\" vs \"How to fix authentication bug in FastAPI?\")\n",
        "- Query transformation help? (HyDE, query expansion)\n",
        "\n",
        "**Check prompt:**\n",
        "- Clear instruction? (\"Use only retrieved context\")\n",
        "- Chunks properly injected? (verify format)\n",
        "- Does model ignore retrieval? (try with/without retrieval)\n",
        "\n",
        "**Check model:**\n",
        "- Run without retrieval â†’ does it hallucinate?\n",
        "- Temperature too high? (try 0.1)\n",
        "- Model too small? (7B struggles vs 70B)\n",
        "\n",
        "**Debugging tools:**\n",
        "- Log all retrieved chunks + scores\n",
        "- Similarity score distribution (should vary, not all 0.8)\n",
        "- Try different retrievers (BM25 vs embedding)\n",
        "\n",
        "***\n",
        "\n",
        "## Vector Databases & Search\n",
        "\n",
        "### What are common causes of low recall in vector search?\n",
        "\n",
        "**Poor embeddings:** Generic model (all-MiniLM-L6) doesn't capture domain nuances (medical, legal)\n",
        "\n",
        "**Embedding drift:** Query embedding model â‰  document embedding model (or different versions)\n",
        "\n",
        "**Wrong distance metric:** Using L2 when embeddings designed for cosine\n",
        "\n",
        "**ANN index too aggressive:** HNSW with `ef_search=10` trades recall for speed (try 50-100)\n",
        "\n",
        "**Query too short/vague:** \"error\" â†’ not enough signal for good embedding\n",
        "\n",
        "**Document preprocessing:** Missing important text, bad chunking\n",
        "\n",
        "**Vocabulary mismatch:** Domain jargon not in embedding model training data\n",
        "\n",
        "**Vector dimensionality:** 384-dim may lose information vs 768-dim\n",
        "\n",
        "**Diagnosis:**\n",
        "- Compare vector search vs BM25 recall\n",
        "- Manually inspect missed documents\n",
        "- Visualize embeddings (t-SNE/UMAP - are clusters meaningful?)\n",
        "- A/B test embedding models\n",
        "\n",
        "***\n",
        "\n",
        "### Why does semantic similarity fail for some queries?\n",
        "\n",
        "**Exact match needed:** \"Python 3.11 bug\" needs keyword, not semantic (3.10 semantically similar but wrong)\n",
        "\n",
        "**Negation:** \"not Windows\" vs \"Windows\" have similar embeddings (negation lost)\n",
        "\n",
        "**Numbers/dates:** \"2023 report\" vs \"2024 report\" embed similarly (temporal info lost)\n",
        "\n",
        "**Acronyms:** \"ML\" vs \"Machine Learning\" may not be close depending on tokenization\n",
        "\n",
        "**Multi-intent:** \"Compare X and Y\" needs both X and Y, semantic search may retrieve only X\n",
        "\n",
        "**Domain-specific:** \"Bullish sentiment\" in finance â‰  general \"positive\"\n",
        "\n",
        "**Solutions:**\n",
        "- Hybrid search (BM25 + semantic)\n",
        "- Query preprocessing (expand acronyms, extract entities)\n",
        "- Fine-tune embeddings on domain data\n",
        "- Metadata filtering (date ranges, exact fields)\n",
        "\n",
        "***\n",
        "\n",
        "### How do you choose between ANN algorithms (HNSW, IVF, etc.)?\n",
        "\n",
        "**HNSW (Hierarchical Navigable Small World):**\n",
        "- **Pros:** Best recall-speed tradeoff, query time fast\n",
        "- **Cons:** High memory (stores full graph), slow indexing\n",
        "- **Use when:** Memory abundant, query latency critical\n",
        "- **Params:** `ef_construction=200, M=16` (higher = better recall, slower)\n",
        "\n",
        "**IVF (Inverted File Index):**\n",
        "- **Pros:** Lower memory, faster indexing\n",
        "- **Cons:** Needs training step, recall-speed tradeoff\n",
        "- **Use when:** Large datasets (>10M vectors), memory constrained\n",
        "- **Params:** `nlist` (clusters), `nprobe` (clusters to search)\n",
        "\n",
        "**Flat (exact search):**\n",
        "- **Pros:** Perfect recall\n",
        "- **Cons:** Slow (\\(O(n)\\) per query)\n",
        "- **Use when:** <100K vectors, or as baseline\n",
        "\n",
        "**Product Quantization (PQ):**\n",
        "- **Pros:** Extreme compression (32Ã— smaller)\n",
        "- **Cons:** Recall loss\n",
        "- **Use when:** Billions of vectors, memory severely constrained\n",
        "\n",
        "**Decision tree:**\n",
        "- <100K vectors â†’ Flat\n",
        "- 100K-10M, memory OK â†’ HNSW\n",
        "- 10M-1B â†’ IVF or HNSW with PQ\n",
        "- >1B â†’ IVF + PQ\n",
        "\n",
        "***\n",
        "\n",
        "### What causes index performance to degrade over time?\n",
        "\n",
        "**Index fragmentation:** Many incremental updates create inefficient structure (like disk fragmentation)\n",
        "\n",
        "**Imbalanced partitions:** IVF clusters become skewed (some large, some empty)\n",
        "\n",
        "**Memory pressure:** Cache misses increase as index grows beyond RAM\n",
        "\n",
        "**Stale statistics:** ANN algorithms rely on distribution assumptions that drift\n",
        "\n",
        "**No retraining:** IVF trained on initial 1M vectors, now has 10M (centroids outdated)\n",
        "\n",
        "**Too many deletions:** Tombstone records accumulate (marked deleted but still in index)\n",
        "\n",
        "**Solutions:**\n",
        "- Rebuild index periodically (weekly/monthly)\n",
        "- Retrain IVF centroids on fresh sample\n",
        "- Merge segments (like Elasticsearch)\n",
        "- Monitor query latency metrics\n",
        "\n",
        "***\n",
        "\n",
        "## Distributed Training & Optimization\n",
        "\n",
        "### Why does mixed precision training work?\n",
        "\n",
        "**FP16 benefits:**\n",
        "- 2Ã— less memory (activations, gradients)\n",
        "- 2-3Ã— faster on modern GPUs (Tensor Cores)\n",
        "- Larger batch sizes fit\n",
        "\n",
        "**Problem:** FP16 range too small â†’ gradients underflow to zero\n",
        "\n",
        "**Solution (Loss Scaling):**\n",
        "1. Scale loss by large factor (e.g., 1024)\n",
        "2. Backward pass: gradients scaled up â†’ don't underflow\n",
        "3. Before optimizer step: scale gradients back down\n",
        "4. Update weights\n",
        "\n",
        "**Math:**\n",
        "\\[\n",
        "\\text{grad}_{scaled} = 1024 \\times \\text{grad}_{actual}\n",
        "\\]\n",
        "\\[\n",
        "\\text{grad}_{final} = \\text{grad}_{scaled} / 1024\n",
        "\\]\n",
        "\n",
        "**BFloat16 (better):**\n",
        "- Same exponent range as FP32 (8 bits)\n",
        "- No loss scaling needed\n",
        "- Slightly less precise mantissa (7 bits vs 10 in FP16)\n",
        "- Used in TPUs, A100 GPUs\n",
        "\n",
        "***\n",
        "\n",
        "### What is gradient accumulation and when do you use it?\n",
        "\n",
        "**Idea:** Simulate large batch by accumulating gradients over \\(N\\) small batches\n",
        "\n",
        "**Implementation:**\n",
        "```python\n",
        "for i, batch in enumerate(loader):\n",
        "    loss = model(batch) / N  # normalize by accumulation steps\n",
        "    loss.backward()  # accumulate gradients\n",
        "    \n",
        "    if (i+1) % N == 0:\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "```\n",
        "\n",
        "**Effect:** Effective batch = \\(N \\times \\text{batch\\_size}\\)\n",
        "\n",
        "**When to use:**\n",
        "- GPU memory limited (can't fit large batch)\n",
        "- Want large effective batch for stability\n",
        "- Distributed training with few GPUs\n",
        "\n",
        "**Trade-offs:**\n",
        "- Slower wall-clock time (N forward/backward before update)\n",
        "- Identical gradient to true large batch (mathematically equivalent)\n",
        "- BatchNorm breaks (but transformers use LayerNorm, so fine)\n",
        "\n",
        "***\n",
        "\n",
        "### Why does learning rate warmup help?\n",
        "\n",
        "**Problem:** At start of training, weights random â†’ gradients large and chaotic\n",
        "\n",
        "**Large LR at start:** Can cause:\n",
        "- Exploding gradients\n",
        "- Divergence\n",
        "- Poor optimization trajectory (get stuck in bad local minimum)\n",
        "\n",
        "**Warmup:** Linearly increase LR from 0 to target over first \\(N\\) steps\n",
        "\n",
        "**Math:**\n",
        "\\[\n",
        "\\text{LR}(step) = \\text{LR}_{max} \\times \\frac{step}{warmup\\_steps}\n",
        "\\]\n",
        "\n",
        "**Why it works:**\n",
        "- Small LR initially â†’ stable updates while model finds general direction\n",
        "- Gradually increase â†’ speeds up once stabilized\n",
        "\n",
        "**Typical:** 2-10% of total training steps (e.g., 1000 steps warmup for 10K total)\n",
        "\n",
        "**Especially important for:**\n",
        "- Very large models (billions of parameters)\n",
        "- Large batch sizes (>1024)\n",
        "- Adam optimizer (adaptive LR can be unstable early)\n",
        "\n",
        "***\n",
        "\n",
        "### What is gradient clipping and when is it necessary?\n",
        "\n",
        "**Definition:** Cap gradient norm to maximum value\n",
        "\n",
        "**Implementation:**\n",
        "```python\n",
        "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "```\n",
        "\n",
        "**Math:**\n",
        "\\[\n",
        "\\text{if } ||\\nabla|| > \\text{max\\_norm}: \\quad \\nabla = \\nabla \\times \\frac{\\text{max\\_norm}}{||\\nabla||}\n",
        "\\]\n",
        "\n",
        "**When necessary:**\n",
        "- **RNNs/LSTMs:** Gradients multiply across time steps â†’ explode\n",
        "- **Very deep networks:** Without residual connections\n",
        "- **Reinforcement learning:** Policy gradients can spike\n",
        "- **Mixed precision training:** Rare but extreme outliers\n",
        "- **Long sequences:** Transformers on 4K+ token sequences\n",
        "\n",
        "**Typical values:**\n",
        "- Conservative: `max_norm=0.5`\n",
        "- Standard: `max_norm=1.0`\n",
        "- Loose: `max_norm=5.0`\n",
        "\n",
        "**Check if needed:** Log gradient norms - if seeing values >10 frequently, enable clipping\n",
        "\n",
        "***\n",
        "\n",
        "### How does learning rate scheduling improve convergence?\n",
        "\n",
        "**Constant LR problems:**\n",
        "- Too high â†’ oscillates around minimum, never settles\n",
        "- Too low â†’ slow convergence\n",
        "\n",
        "**Scheduling:** Reduce LR during training\n",
        "\n",
        "**Common schedules:**\n",
        "\n",
        "**Cosine annealing:**\n",
        "$\n",
        "\\text{LR}(t) = \\text{LR}_{min} + \\frac{1}{2}(\\text{LR}_{max} - \\text{LR}_{min})(1 + \\cos(\\frac{t\\pi}{T}))\n",
        "$\n",
        "- Smooth decrease\n",
        "- Used in GPT-3, LLaMA\n",
        "\n",
        "**Step decay:**\n",
        "$\n",
        "\\text{LR}(epoch) = \\text{LR}_{init} \\times 0.1^{\\lfloor epoch/30 \\rfloor}\n",
        "$\n",
        "- Drop by 10Ã— every 30 epochs\n",
        "- Simple, effective for CNNs\n",
        "\n",
        "**Inverse square root:**\n",
        "$\n",
        "\\text{LR}(t) = \\text{LR}_{init} / \\sqrt{t}\n",
        "$\n",
        "- Used in original Transformer\n",
        "\n",
        "**Why it works:**\n",
        "- High LR early â†’ fast exploration\n",
        "- Low LR late â†’ fine-tune near minimum\n",
        "\n",
        "**With warmup:** Combine both (warmup + cosine annealing) for best results\n",
        "\n",
        "***\n"
      ],
      "metadata": {
        "id": "z9nhdsH7mUqs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set 2"
      ],
      "metadata": {
        "id": "ItjlTQO3fI7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QnA - 1\n",
        "\n",
        "## ðŸŸ¢ P0 â€“ Core ML, Training & Debugging"
      ],
      "metadata": {
        "id": "hjHRK5iLhVzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "### Data & Generalization\n",
        "\n",
        "**1. Why can adding more data reduce performance?**\n",
        "\n",
        "**Math insight**: Model minimizes empirical risk $R_{emp}(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} L(f_\\theta(x_i), y_i)$. When new data changes the underlying distribution $P(x,y)$, optimizing on mixed distribution can hurt:\n",
        "- If $P_{new}(y|x)$ conflicts with $P_{old}(y|x)$, gradients fight each other\n",
        "- Rare but important examples have weight $\\frac{1}{n}$ â†’ as $n$ grows, weight â†’ 0\n",
        "\n",
        "**Key reasons**:\n",
        "- **Distribution mismatch**: New data from different sources/labeling standards ($P_{old} \\neq P_{new}$)\n",
        "- **Capacity constraint**: Model has fixed VC dimensionâ€”can't capture added complexity\n",
        "- **Signal dilution**: Important patterns get weight $\\frac{1}{n}$ in loss, swamped by noise\n",
        "\n",
        "**Solution**: Check KL-divergence or JS-divergence between old/new distributions. For mixed sources (Amazon reviews + Twitter), use importance weighting: $w_i = \\frac{P_{target}(x_i)}{P_{train}(x_i)}$. Monitor per-source metrics separately.\n",
        "\n",
        "---\n",
        "\n",
        "**2. How do you detect covariate shift without labels?**\n",
        "\n",
        "**Math**: Testing if $P_{train}(X) \\neq P_{deploy}(X)$ while $P(Y|X)$ stays same\n",
        "\n",
        "**Methods**:\n",
        "- **Discriminator approach**: Train classifier $D(x): \\{train, deploy\\}$. If accuracy $> 0.75$ â†’ $H_1$: distributions differ\n",
        "- **Statistical tests**: Two-sample tests\n",
        "  - Kolmogorov-Smirnov: $D = \\sup_x |F_{train}(x) - F_{deploy}(x)|$\n",
        "  - Maximum Mean Discrepancy: $||E_{P_{train}}[\\phi(x)] - E_{P_{deploy}}[\\phi(x)]||_H$\n",
        "- **Monitor drift**: Track $H(p)$ (entropy) or $KL(p_{old} || p_{new})$ of prediction distributions over time\n",
        "\n",
        "**Solution**: Start with RandomForest discriminatorâ€”feature importance shows which features shift. Validate with statistical tests on top shifters. Use PCA visualization for exploration.\n",
        "\n",
        "---\n",
        "\n",
        "**3. When prefer high-variance, low-bias models?**\n",
        "\n",
        "**Math**: Bias-variance decomposition of MSE:\n",
        "$E[(y - \\hat{f}(x))^2] = Bias[\\hat{f}(x)]^2 + Var[\\hat{f}(x)] + \\sigma^2$\n",
        "where $Var[\\hat{f}(x)] \\propto \\frac{1}{\\sqrt{n}}$ (decreases with data)\n",
        "\n",
        "**Use when**:\n",
        "- **Large data**: Law of large numbers tames varianceâ€”with $n$ samples, variance decreases as $O(1/\\sqrt{n})$\n",
        "- **Ensemble available**: Bagging reduces variance by $\\frac{1}{M}$ with $M$ uncorrelated models\n",
        "- **Asymmetric costs**: Systematic bias (missing demographics) worse than random errors\n",
        "\n",
        "**Solution**: Fraud detection with millions of transactions â†’ use deep networks over logistic regression. Volume reduces variance, but no data fixes systematic bias.\n",
        "\n",
        "---\n",
        "\n",
        "**4. Why does shuffling matter during training?**\n",
        "\n",
        "**Math**: SGD assumes mini-batches are i.i.d. samples from $P(X,y)$. Update rule:\n",
        "$\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta \\frac{1}{B} \\sum_{i \\in \\text{batch}} L(f_\\theta(x_i), y_i)$\n",
        "Without shuffling, batches are temporally correlated â†’ $Cov(\\nabla_t, \\nabla_{t+1}) \\neq 0$ â†’ biased updates\n",
        "\n",
        "**Without shuffling**:\n",
        "- Sequential batches â†’ $E[\\nabla_t | \\nabla_{t-1}]$ correlated â†’ oscillating convergence\n",
        "- BatchNorm: $\\mu_B, \\sigma_B$ biased by temporal ordering\n",
        "- Model learns sequence artifacts: $P(y|x, \\text{position})$ instead of $P(y|x)$\n",
        "\n",
        "**Solution**: Shuffle once per epoch. For time-series, use proper temporal splits (no shuffling). Check if gradient variance between epochs is highâ€”indicates ordering issues.\n",
        "\n",
        "---\n",
        "\n",
        "**5. What if train/validation distributions differ?**\n",
        "\n",
        "**Math**: Hyperparameter selection optimizes $\\lambda^* = \\arg\\min_\\lambda L_{val}(\\theta_\\lambda)$. But if $P_{val}(x,y) \\neq P_{deploy}(x,y)$, then $\\lambda^*_{val} \\neq \\lambda^*_{deploy}$\n",
        "\n",
        "**Impact**:\n",
        "- Hyperparameter selection unreliable: optimizing wrong objective\n",
        "- Early stopping fails: stops at $\\arg\\min_t L_{val}(t)$ instead of $\\arg\\min_t L_{deploy}(t)$\n",
        "- Performance estimates biased: $E_{P_{val}}[L] \\neq E_{P_{deploy}}[L]$\n",
        "\n",
        "**Solution**: Validation must mirror deployment. Use stratified splits. Check distribution stats (mean, std, quantiles). If validation uses recent/clean data, estimates will be overoptimistic.\n",
        "\n",
        "---\n",
        "\n",
        "### Optimization Behavior\n",
        "\n",
        "**6. Why do gradients shrink while loss drops?**\n",
        "\n",
        "**Math**: Near local minimum, first-order optimality: $\\nabla_\\theta L \\to 0$. Gradient magnitude: $||\\nabla L|| \\approx ||\\nabla^2 L|| \\cdot ||s||$ where $s$ is distance to minimum (by Taylor expansion)\n",
        "\n",
        "As we approach minimum: $||s|| \\to 0$ â†’ $||\\nabla L|| \\to 0$ even if loss still decreasing\n",
        "\n",
        "**Normal**: Approaching minimum â†’ flatter landscape (lower curvature) â†’ smaller gradients. Flat minima correlate with better generalization (Hochreiter & Schmidhuber)\n",
        "\n",
        "**Problem**:\n",
        "- Vanishing gradients from saturated activations: $\\sigma'(z) \\to 0$ for $|z| \\gg 0$ (sigmoid/tanh)\n",
        "- Poor conditioning: $\\kappa(H) = \\frac{\\lambda_{max}}{\\lambda_{min}} \\gg 1$ â†’ some directions have tiny gradients\n",
        "\n",
        "**Solution**: Plot $||\\nabla \\theta||$ over time. Smooth exponential decay = healthy. Sudden drop = check activation distributions (use ReLU: $\\frac{\\partial}{\\partial z} \\max(0,z) = 1$ for $z>0$), adjust learning rate, or verify loss still improving.\n",
        "\n",
        "---\n",
        "\n",
        "**7. What causes training instability after convergence?**\n",
        "\n",
        "**Common causes**:\n",
        "- Learning rate too high for refined optimization\n",
        "- BatchNorm with small batches â†’ noisy statistics\n",
        "- Momentum overshooting good solutions\n",
        "- Mixed precision accumulation errors\n",
        "\n",
        "**Solution**: Use learning rate decay (cosine annealing). Apply gradient clipping (1.0-5.0). Switch to GroupNorm for small batches. Check FP16 loss scaling.\n",
        "\n",
        "---\n",
        "\n",
        "**8. Why does warm-up help large models?**\n",
        "\n",
        "**Math**: Early gradients are noisy: $g_t = \\nabla L + \\epsilon_t$ where $||\\epsilon_t|| \\gg ||\\nabla L||$ with random init.\n",
        "\n",
        "Signal-to-noise ratio: $SNR = \\frac{||\\nabla L||}{||\\epsilon_t||} \\approx O(\\frac{1}{\\sqrt{d}})$ for $d$ parameters\n",
        "\n",
        "With full learning rate: $||\\Delta \\theta|| = \\eta ||g_t|| \\approx \\eta ||\\epsilon_t||$ â†’ large random updates\n",
        "\n",
        "**Problem**: Full LR with noisy gradients â†’ destabilizes training, explodes gradients (especially in deep networks where $||\\nabla L^{(l)}|| = \\prod_{i>l} ||J_i|| \\cdot ||\\nabla L^{(L)}||$), locks into poor minima\n",
        "\n",
        "**Solution**: Warm-up linearly increases LR: $\\eta_t = \\eta_{max} \\cdot \\frac{t}{T_{warmup}}$ for $t \\leq T_{warmup}$. Start small ($10^{-7}$), ramp over 1-10% of training. Critical for Adam and large batch sizes where noise compounds: effective gradient $\\approx \\frac{1}{B}\\sum g_i$ has variance $\\frac{\\sigma^2}{B}$.\n",
        "\n",
        "---\n",
        "\n",
        "**9. Weight decay vs L2 regularizationâ€”what's the difference?**\n",
        "\n",
        "**Math**:\n",
        "- **L2 regularization**: $L_{total} = L(f_\\theta(x), y) + \\frac{\\lambda}{2}||\\theta||^2$, then compute $\\nabla_\\theta L_{total}$\n",
        "- **Weight decay**: Update $\\theta \\leftarrow (1-\\lambda)\\theta - \\eta \\nabla_\\theta L$\n",
        "\n",
        "**For SGD**: Equivalent since $\\nabla(L + \\frac{\\lambda}{2}||\\theta||^2) = \\nabla L + \\lambda \\theta$\n",
        "\n",
        "**For Adam**: Different! Adam uses adaptive learning rates:\n",
        "$m_t = \\beta_1 m_{t-1} + (1-\\beta_1)g_t, \\quad v_t = \\beta_2 v_{t-1} + (1-\\beta_2)g_t^2$\n",
        "$\\theta_{t+1} = \\theta_t - \\eta \\frac{m_t}{\\sqrt{v_t} + \\epsilon}$\n",
        "\n",
        "With L2, gradient $g_t = \\nabla L + \\lambda \\theta$ gets rescaled by $\\frac{1}{\\sqrt{v_t}}$ â†’ heavily-updated params get less regularization\n",
        "\n",
        "**Solution**: Use AdamW (Adam + weight decay): $\\theta \\leftarrow (1-\\lambda)\\theta - \\eta \\frac{m_t}{\\sqrt{v_t}}$. Applies uniform regularization independent of adaptive scaling.\n",
        "\n",
        "---\n",
        "\n",
        "**10. Why do models learn spurious correlations faster?**\n",
        "\n",
        "**Math**: SGD follows steepest descent with respect to loss $L$. Given two features $x_1$ (spurious), $x_2$ (causal), if $||\\nabla_{x_1} L|| > ||\\nabla_{x_2} L||$ initially, model learns $x_1$ first (simplicity bias / implicit regularization)\n",
        "\n",
        "**Reasons**:\n",
        "- Spurious patterns have simpler decision boundaries (lower VC dimension)\n",
        "- Local features (texture) vs global (shape): $\\nabla_{texture} L$ computable from small receptive fields\n",
        "- Dataset artifacts highly predictive: $I(y; \\text{artifact}) > I(y; \\text{causal})$ in training data\n",
        "- True causality requires multi-hop: $P(y|do(x))$ vs $P(y|x)$\n",
        "\n",
        "**Solution**: Data augmentation to remove shortcuts (e.g., ColoredMNIST with random colors). Validate on out-of-distribution data where spurious correlations break. Use counterfactual examples. Check feature attribution (GradCAM, SHAP) to see what drives predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### Neural Network Internals\n",
        "\n",
        "**11. Why do deep networks need normalization?**\n",
        "\n",
        "**Math**: Internal covariate shiftâ€”layer inputs change distribution as params update\n",
        "\n",
        "**Problems**: Each layer adapts to moving targets, gradients vanish/explode exponentially, learning rate hard to tune (different scales per layer)\n",
        "\n",
        "**Solution**: BatchNorm stabilizes distributions, enables higher learning rates, acts as implicit regularization. Use LayerNorm for small batches or sequential data. GroupNorm for very small batches.\n",
        "\n",
        "---\n",
        "\n",
        "**12. What causes dead neurons (ReLU)?**\n",
        "\n",
        "**Math**: ReLU activation $a = \\max(0, z)$ where $z = w^Tx + b$.\n",
        "\n",
        "Gradient: $\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} \\cdot x = \\frac{\\partial L}{\\partial a} \\cdot \\mathbb{1}_{z>0} \\cdot x$\n",
        "\n",
        "If $z = w^Tx + b < 0$ for all inputs â†’ $\\frac{\\partial a}{\\partial z} = 0$ always â†’ no gradient â†’ stuck permanently (dying ReLU)\n",
        "\n",
        "**Causes**:\n",
        "- Large negative bias initialization\n",
        "- Aggressive learning rate: update $\\Delta w = -\\eta g$ pushes $w$ such that $w^Tx + b < 0$ for all $x$ in data manifold\n",
        "- Once dead, $\\nabla w = 0$ â†’ no recovery mechanism\n",
        "\n",
        "**Solution**: Use LeakyReLU ($a = \\max(0.01z, z)$) or PReLU ($a = \\max(\\alpha z, z)$ where $\\alpha$ is learned). Apply He initialization: $w \\sim N(0, \\frac{2}{n_{in}})$. Reduce LR. Monitor dead neuron fraction: $\\frac{1}{N}\\sum_{i=1}^N \\mathbb{1}_{a_i = 0}$ during training.\n",
        "\n",
        "---\n",
        "\n",
        "**13. Why does initialization matter more for deep models?**\n",
        "\n",
        "**Math**: Forward pass variance propagation. For layer $l$:\n",
        "$y^{(l)} = W^{(l)} x^{(l-1)} + b^{(l)}$\n",
        "$Var(y^{(l)}_i) = n_{l-1} \\cdot Var(w^{(l)}_{ij}) \\cdot Var(x^{(l-1)}_j)$\n",
        "\n",
        "If $Var(y^{(l)}) = c \\cdot Var(x^{(l-1)})$, then after $L$ layers: $Var(y^{(L)}) = c^L \\cdot Var(x^{(0)})$\n",
        "- If $c > 1$: activations explode ($c^L \\to \\infty$)\n",
        "- If $c < 1$: activations vanish ($c^L \\to 0$)\n",
        "\n",
        "Backward pass: $\\frac{\\partial L}{\\partial x^{(l)}} = (W^{(l+1)})^T \\frac{\\partial L}{\\partial x^{(l+1)}}$ â†’ same exponential effect on gradients\n",
        "\n",
        "**Impact**: Poor init â†’ signals/gradients unusable. Symmetry breaking requiredâ€”identical weights â†’ identical gradients â†’ neurons never differentiate\n",
        "\n",
        "**Solution**:\n",
        "- Xavier: $w \\sim N(0, \\frac{2}{n_{in} + n_{out}})$ for tanh/sigmoid (maintains $Var = 1$)\n",
        "- He: $w \\sim N(0, \\frac{2}{n_{in}})$ for ReLU (accounts for half neurons dead)\n",
        "\n",
        "---\n",
        "\n",
        "**14. Why does dropout sometimes hurt?**\n",
        "\n",
        "**Use dropout when**: Model overfits, have sufficient capacity, training for many epochs\n",
        "\n",
        "**Skip dropout when**:\n",
        "- Model already simple/small (underfitting)\n",
        "- Using BatchNorm (they interact poorlyâ€”use one or other)\n",
        "- Small dataset where model needs full capacity\n",
        "- Insufficient training time for ensemble effect\n",
        "\n",
        "**Solution**: Modern architectures often skip dropout in favor of other regularization (weight decay, data augmentation, BatchNorm). Test with/without on validation set.\n",
        "\n",
        "---\n",
        "\n",
        "**15. How do residual connections help?**\n",
        "\n",
        "**Math**: $y = F(x) + x$ creates gradient highway: $âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚y Â· (âˆ‚F/âˆ‚x + I)$\n",
        "\n",
        "**Benefits**:\n",
        "- Gradient flows directly to early layers (prevents vanishing)\n",
        "- Model learns incremental refinements (identity + Î”) not full transformations\n",
        "- Creates ensemble of different-depth sub-networks\n",
        "- Smooths loss landscape\n",
        "\n",
        "**Solution**: Use ResNet-style skip connections for ```networks >20``` layers. Enables training ```50-200+ layer networks``` that would otherwise be impossible.\n",
        "\n",
        "---\n",
        "\n",
        "### Practical Debugging\n",
        "\n",
        "**16. How debug high-confidence but wrong predictions?**\n",
        "\n",
        "**Math**: Calibration issueâ€”P(correct|confidence) â‰  confidence\n",
        "\n",
        "**Check**:\n",
        "- Dataset leakage or train/test contamination\n",
        "- Spurious correlation exploitation (visualize high-confidence errors)\n",
        "- Label quality for those examples\n",
        "- Overfitting to specific patterns\n",
        "\n",
        "**Solution**: Apply temperature scaling post-training. Use ensembles or MC dropout for uncertainty. Examine attention/activations on confident mistakes. Check reliability diagrams (calibration plots).\n",
        "\n",
        "---\n",
        "\n",
        "**17. Why does adding features sometimes hurt performance?**\n",
        "\n",
        "**Math**: Curse of dimensionality. In $d$-dimensional space with $n$ samples:\n",
        "- Volume of hypercube: $V \\propto r^d$ â†’ grows exponentially\n",
        "- Sample density: $\\rho = \\frac{n}{r^d} \\propto \\frac{n}{r^d}$ â†’ for fixed $n$, $\\rho \\to 0$ as $d$ grows\n",
        "- Distance concentration: $\\frac{d_{max} - d_{min}}{d_{min}} \\to 0$ as $d \\to \\infty$ (all points equidistant)\n",
        "\n",
        "**Problems**:\n",
        "- Sparse coverage: need $n = O(r^d)$ samples to maintain density (exponential in $d$)\n",
        "- Noise features dilute SNR: $SNR = \\frac{\\text{signal variance}}{\\text{noise variance}} = \\frac{k \\sigma_s^2}{(d-k)\\sigma_n^2}$ where $k$ is informative features\n",
        "- Spurious correlations: $O(d^2)$ pairwise correlations, many random\n",
        "- Overfitting easier: VC dimension grows with features\n",
        "\n",
        "**Solution**: Feature selection (Lasso: $\\min_w ||y - Xw||^2 + \\lambda ||w||_1$, tree-based importance). Dimensionality reduction (PCA: $X_{reduced} = XV_k$ where $V_k$ are top $k$ eigenvectors). Or collect more data: $n \\gg d$.\n",
        "\n",
        "---\n",
        "\n",
        "**18. Underfitting vs noisy dataâ€”how to tell?**\n",
        "\n",
        "**Underfitting signs**:\n",
        "- Train loss high and plateaued\n",
        "- Both train and val poor\n",
        "- Increasing capacity helps\n",
        "- Training longer doesn't help\n",
        "\n",
        "**Noisy data signs**:\n",
        "- Train loss decreases but val plateaus\n",
        "- Individual examples inconsistent\n",
        "- Human performance poor on task\n",
        "- Cleaning/relabeling subset significantly improves val\n",
        "\n",
        "**Solution**: Check Bayes error (human performance). If high â†’ noisy data. If low but model poor â†’ underfitting. Measure annotator agreement.\n",
        "\n",
        "---\n",
        "\n",
        "**19. How validate preprocessing pipeline?**\n",
        "\n",
        "**Critical checks**:\n",
        "- Visualize transformed samplesâ€”sensible?\n",
        "- Apply inverse transform â†’ verify reconstruction\n",
        "- Check statistics (mean, std, range) at each stage\n",
        "- Test edge cases (empty, outliers, missing values)\n",
        "- Compare train vs val distributions post-preprocessing\n",
        "- Train simple modelâ€”if can't learn anything, preprocessing likely broken\n",
        "\n",
        "**Solution**: Unit test each transformation. Save before/after examples. Create synthetic test cases with known outputs.\n",
        "\n",
        "---\n",
        "\n",
        "**20. Why does training differ on GPU vs CPU?**\n",
        "\n",
        "**Causes**:\n",
        "- Precision differences (FP16 vs FP32)\n",
        "- Non-deterministic operations (cuDNN algorithms)\n",
        "- Parallel accumulation order varies\n",
        "- Memory layout differences\n",
        "\n",
        "**Solution**: Set deterministic flags and seeds, but expect small numerical differences. Large divergence indicates instabilityâ€”check mixed precision settings, gradient clipping, or condition number.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "tgtKF4lHfSi-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QnA -2\n",
        "## ðŸŸ¡ P1 â€“ LLMs, Generative Models & Failure Modes"
      ],
      "metadata": {
        "id": "G78istoRhvU4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "### Model Behavior & Limitations\n",
        "\n",
        "**21. Why do LLMs struggle with multi-step reasoning?**\n",
        "\n",
        "**Math**: Single forward pass generates tokens autoregressively: ```P(y_t | y_<t,x)```\n",
        "\n",
        "**Limitations**:\n",
        "- No iteration or backtrackingâ€”each token independent\n",
        "- No working memory for intermediate steps\n",
        "- Errors compoundâ€”one wrong step derails everything\n",
        "- Pattern matching, not symbolic logic execution\n",
        "- Long reasoning chains exceed effective context window\n",
        "\n",
        "**Solution**: Prompt for explicit chain-of-thought. Use tool-calling for computation (calculator, code execution). Implement multi-turn refinement. Break complex reasoning into smaller steps.\n",
        "\n",
        "---\n",
        "\n",
        "**22. What causes repetition loops?**\n",
        "\n",
        "**Sampling trapped in high-probability cycles**: ```p(token | context_with_repetition) â†’ same token â†’ reinforces pattern```\n",
        "\n",
        "**Triggers**:\n",
        "- Temperature too low â†’ deterministic sampling\n",
        "- Context fills with pattern â†’ attention focuses on it\n",
        "- Training data contained repetitive patterns\n",
        "\n",
        "**Solution**: Apply repetition penalty (penalize recently used tokens). Increase temperature slightly. Use nucleus sampling (top-p) instead of greedy. Set presence/frequency penalties to break cycles.\n",
        "\n",
        "---\n",
        "\n",
        "**23. Why do LLMs ignore system prompts?**\n",
        "\n",
        "**Competition**: System prompts compete with strong training priors\n",
        "\n",
        "**Reasons**:\n",
        "- Training data patterns overpower weak system instructions\n",
        "- Instruction-tuning didn't cover diverse system prompt formats\n",
        "- User content more salient than system context\n",
        "- System prompts lost in long conversations\n",
        "\n",
        "**Solution**: Make system prompts specific and concise. Reinforce key constraints in user messages. Use few-shot examples. Keep conversations focused to maintain system context.\n",
        "\n",
        "---\n",
        "\n",
        "**24. How does sampling affect diversity vs correctness?**\n",
        "\n",
        "**Math**: Sample from $P(token|context)$ with temperature $\\tau$:\n",
        "$P_\\tau(w_i) = \\frac{\\exp(z_i/\\tau)}{\\sum_j \\exp(z_j/\\tau)}$\n",
        "where $z_i$ are logits. As $\\tau \\to 0$: $P_\\tau \\to$ one-hot (argmax). As $\\tau \\to \\infty$: $P_\\tau \\to$ uniform\n",
        "\n",
        "**Strategies**:\n",
        "- **Low temp ($\\tau = 0.1-0.3$)**: Sharpens distribution â†’ high correctness, low diversity (deterministic argmax)\n",
        "- **High temp ($\\tau = 0.8-1.2$)**: Flattens distribution â†’ high diversity, lower correctness (samples unlikely tokens)\n",
        "- **Top-p (nucleus)**: Sample from smallest set $V$ where $\\sum_{w \\in V} P(w) \\geq p$. Adaptive cutoff\n",
        "- **Top-k**: Sample from top $k$ highest probability tokens\n",
        "\n",
        "Expected entropy: $H = -\\sum_i P_\\tau(w_i) \\log P_\\tau(w_i)$ increases with $\\tau$\n",
        "\n",
        "**Solution**: Factual tasks â†’ low temp. Creative tasks â†’ higher temp. Use nucleus ($p=0.9$) for balance. Beam search (keep top-$k$ sequences at each step) for coherence but lower diversity.\n",
        "\n",
        "---\n",
        "\n",
        "**25. When doesn't scaling help downstream performance?**\n",
        "\n",
        "**Reasons**:\n",
        "- Task doesn't require emergent capabilities\n",
        "- Larger models overfit to pretraining biases\n",
        "- Downstream data too small (overfits)\n",
        "- Evaluation measures superficial patterns small models capture\n",
        "- Larger models need more careful prompting\n",
        "\n",
        "**Solution**: Right-size models to tasks. Improve prompting before scaling. Test if task benefits from scale (reasoning vs pattern matching). Consider efficiencyâ€”small tuned model often beats large zero-shot.\n",
        "\n",
        "---\n",
        "\n",
        "### Context & Memory\n",
        "\n",
        "**26. Why do LLMs \"forget\" long context?**\n",
        "\n",
        "**Math**: Self-attention computes attention weights via softmax over sequence length $N$:\n",
        "$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$\n",
        "$\\alpha_{ij} = \\frac{\\exp(q_i \\cdot k_j / \\sqrt{d_k})}{\\sum_{j'=1}^N \\exp(q_i \\cdot k_{j'} / \\sqrt{d_k})}$\n",
        "\n",
        "As $N$ grows large: $\\alpha_{ij} \\approx \\frac{1}{N}$ (each token gets equal weight)\n",
        "\n",
        "**Problems**:\n",
        "- Attention dilution: Individual important tokens receive weight $\\approx \\frac{1}{N}$ â†’ signal lost in noise\n",
        "- Positional encoding degrades: Sinusoidal or learned positions extrapolate poorly beyond training length\n",
        "- Training bias: Most training sequences shorter â†’ long-range dependencies undertrained\n",
        "- Gradient flow: $\\frac{\\partial L}{\\partial x_1} = \\frac{\\partial L}{\\partial x_N} \\cdot \\prod_{l} J_l$ where $J_l$ involves attention â†’ weakens with distance\n",
        "- Computational: $O(N^2)$ memory and compute for attention matrix\n",
        "\n",
        "**Solution**: Summarize important info periodically. Use retrieval (embed + search top-$k$ relevant segments). Re-inject critical information. Sparse/hierarchical attention patterns. Extend positional encodings (RoPE, ALiBi).\n",
        "\n",
        "---\n",
        "\n",
        "**27. What are failure modes of long-context LLMs?**\n",
        "\n",
        "**Common issues**:\n",
        "- **Lost in the middle**: Ignores middle portions (primacy/recency bias)\n",
        "- **Attention dilution**: Each token gets â‰ˆ1/N attention weight\n",
        "- **Conflicting information**: Can't resolve contradictions in long docs\n",
        "- **Fabrication**: Relevant info present but not attended to\n",
        "- **Increased latency/cost**: Quadratic attention complexity\n",
        "\n",
        "**Solution**: Test with needle-in-haystack. Use retrieval to focus on relevant segments. Prompt to consider entire context. Monitor position-dependent accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "**28. Why is retrieval better than just increasing context?**\n",
        "\n",
        "**Math**: Attention complexity O(NÂ²), retrieval O(N log N) with index\n",
        "\n",
        "**Retrieval advantages**:\n",
        "- Focuses attention on relevant information only\n",
        "- Scales better than quadratic attention\n",
        "- Can leverage external, updatable knowledge\n",
        "- Reduces cost (process less tokens)\n",
        "- More interpretableâ€”see what was retrieved\n",
        "\n",
        "**Solution**: Hybrid approach works best: retrieve top-k relevant segments, then reason over them within context window. Embed query + docs, retrieve by similarity, rerank by relevance.\n",
        "\n",
        "---\n",
        "\n",
        "### Alignment & Instruction Following\n",
        "\n",
        "**29. Why does RLHF reduce creativity?**\n",
        "\n",
        "**Math**: RLHF optimizes policy $\\pi_\\theta$ via reward model $r_\\phi$:\n",
        "$\\max_\\theta \\mathbb{E}_{x \\sim P(x), y \\sim \\pi_\\theta(y|x)}[r_\\phi(x,y)] - \\beta \\cdot D_{KL}(\\pi_\\theta || \\pi_{ref})$\n",
        "\n",
        "where $\\pi_{ref}$ is the pretrained base model, $\\beta$ controls how much we deviate\n",
        "\n",
        "**Effect**:\n",
        "- Reward model $r_\\phi$ trained on human preferences â†’ biased toward safe, conventional responses (mode-seeking)\n",
        "- KL constraint $D_{KL}(\\pi_\\theta || \\pi_{ref}) = \\mathbb{E}_{y \\sim \\pi_\\theta}[\\log \\pi_\\theta(y|x) - \\log \\pi_{ref}(y|x)]$ penalizes deviation from base model\n",
        "- Mode collapse: $\\pi_\\theta$ concentrates on high-reward modes, ignoring creative but risky options\n",
        "- Human raters reward \"helpful-looking\" verbose responses â†’ policy learns superficial markers\n",
        "\n",
        "**Why creativity suffers**:\n",
        "- Creative responses have higher variance in human ratings â†’ lower expected reward\n",
        "- Novel outputs penalized by KL term (weren't in $\\pi_{ref}$ distribution)\n",
        "- Risk-averse training: prefer consistent 7/10 over occasional 10/10 or 3/10\n",
        "\n",
        "**Solution**: Adjust KL penalty $\\beta$ (lower $\\beta$ = more exploration). Use DPO (Direct Preference Optimization) which optimizes directly on preferences. Prompt explicitly for creativity. Use base models for creative generation.\n",
        "\n",
        "---\n",
        "\n",
        "**30. How does alignment introduce new biases?**\n",
        "\n",
        "**Sources**:\n",
        "- Demographic skew in rater populations\n",
        "- Cultural preferences in \"helpful\" ratings\n",
        "- Overcompensation creating different biases\n",
        "- Reward hackingâ€”model learns superficial markers\n",
        "- Underrepresentation of certain use cases\n",
        "\n",
        "**Solution**: Diverse rater pool. Red-teaming for edge cases. Monitor fairness metrics across demographics. Continuous evaluation on real use cases. Biases shift, don't disappear.\n",
        "\n",
        "---\n",
        "\n",
        "**31. Why instruction-tuned models worse on raw text?**\n",
        "\n",
        "**Math**: Distribution shiftâ€”model trained on $P(y|instruction, x)$, deployed on $P(y|x)$\n",
        "\n",
        "**Problems**:\n",
        "- Expects structured prompts, not raw completion\n",
        "- Training emphasized Q&A format\n",
        "- Safety tuning causes refusals on ambiguous inputs\n",
        "- Loss of raw language modeling capability\n",
        "\n",
        "**Solution**: Use base models for completion tasks. Use instruct models for interactive tasks. Fine-tune on your specific use case if distribution differs significantly.\n",
        "\n",
        "---\n",
        "\n",
        "**32. Safety vs usefulness tradeoffs?**\n",
        "\n",
        "**Tighter safety**: More false refusals, reduced nuance handling, over-cautious responses\n",
        "\n",
        "**Looser safety**: Potential misuse, harmful content generation, liability issues\n",
        "\n",
        "**Solution**: Precise safety that doesn't overbroadly restrict. Iterative refinement based on real failures. Context-aware safety (medical discussion â‰  instructions to harm). User feedback loops.\n",
        "\n",
        "---\n",
        "\n",
        "**33. Why does instruction following fail on OOD tasks?**\n",
        "\n",
        "**Coverage limitation**: Instruction-tuning data is finite\n",
        "\n",
        "**Causes**:\n",
        "- Novel task formats not in training\n",
        "- Unusual phrasing confuses instruction parsing\n",
        "- Domain-specific jargon not covered\n",
        "- Complex instructions exceed training complexity\n",
        "\n",
        "**Solution**: Use few-shot examples to bridge gap. Provide clearer, more explicit instructions. Decompose complex tasks into familiar subtasks. Fine-tune on domain-specific instruction data.\n",
        "\n",
        "---\n",
        "\n",
        "### Evaluation & Trust\n",
        "\n",
        "**34. Why is \"looks correct\" dangerous?**\n",
        "\n",
        "**LLMs are persuasive â‰  accurate**:\n",
        "- Confident tone doesn't mean correctness\n",
        "- Plausible fabrications (hallucinations) common\n",
        "- Evaluator expertise mattersâ€”non-experts miss subtle errors\n",
        "- Confirmation biasâ€”see what you expect\n",
        "\n",
        "**Solution**: Verify factual claims against sources. Use automated checks for objective criteria. Expert review for domain content. Cross-reference multiple sources. Check reasoning steps, not just final answer.\n",
        "\n",
        "---\n",
        "\n",
        "**35. How detect benchmark over-optimization?**\n",
        "\n",
        "**Signs**:\n",
        "- Benchmark scores improve but real-world performance doesn't\n",
        "- Poor performance on slight benchmark variations\n",
        "- Exploits dataset artifacts (spurious correlations)\n",
        "- Training loss decreases but validation plateaus\n",
        "\n",
        "**Solution**: Hold-out test sets. Use multiple diverse benchmarks. Test adversarial examples. Evaluate on actual use cases. Never train on test distribution. Check if improvements transfer to related tasks.\n",
        "\n",
        "---\n",
        "\n",
        "**36. Why do human evaluators disagree on LLM outputs?**\n",
        "\n",
        "**Subjectivity**: Quality is multifacetedâ€”helpfulness, accuracy, style, tone\n",
        "\n",
        "**Causes**:\n",
        "- Task ambiguityâ€”unclear \"good\" definition\n",
        "- Personal preferences vary\n",
        "- Context-dependent judgments\n",
        "- Evaluator fatigue and attention issues\n",
        "\n",
        "**Solution**: Multiple raters per example. Clear rubrics with examples. Pairwise comparisons (easier than absolute ratings). Measure inter-rater agreement (Cohen's kappa). Expert calibration sessions.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Rfusrk1_hyUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QnA -3\n",
        "## ðŸ”µ P2 â€“ Production & Systems"
      ],
      "metadata": {
        "id": "eNka363HjMZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Infrastructure & Scaling\n",
        "\n",
        "**37. Why does cost grow non-linearly with traffic?**\n",
        "\n",
        "**Step functions in infrastructure**:\n",
        "- Need new servers/regions at thresholds\n",
        "- Load balancers, redundancy complexity\n",
        "- Retrieval index size grows â†’ search slower\n",
        "- Cache hit rates decrease with query diversity\n",
        "- Cross-region replication for reliability\n",
        "- More edge cases need handling\n",
        "\n",
        "**Solution**: Plan for step functions. Use auto-scaling with buffers. Optimize cache strategies. Monitor cost per request. Consider reserved capacity for baseline load.\n",
        "\n",
        "---\n",
        "\n",
        "**38. When is horizontal scaling ineffective?**\n",
        "\n",
        "**Horizontal scaling (more servers) fails when**:\n",
        "- Latency dominated by single-request processing, not throughput\n",
        "- Model too large for single GPUâ€”needs model parallelism\n",
        "- Cross-server communication overhead dominates\n",
        "- Stateful operations require sticky routing (caching, sessions)\n",
        "\n",
        "**Solution**: Model optimization (quantization, distillation). Batch requests efficiently. Vertical scaling with bigger GPUs. Hybrid approachâ€”horizontal for throughput, vertical for latency.\n",
        "\n",
        "---\n",
        "\n",
        "**39. Why does throughput optimization increase errors?**\n",
        "\n",
        "**Quality tradeoffs**:\n",
        "- Larger batches change behavior (BatchNorm effects)\n",
        "- Aggressive quantization loses precision\n",
        "- Reduced context for speed misses information\n",
        "- Caching serves stale results\n",
        "- Faster/smaller models less capable\n",
        "\n",
        "**Solution**: Monitor error rates alongside throughput. Set quality thresholds (SLAs). A/B test optimizations. Don't optimize past acceptable error rate. Use graceful degradation.\n",
        "\n",
        "---\n",
        "\n",
        "**40. CPU vs GPU inferenceâ€”how to choose?**\n",
        "\n",
        "**GPUs win for**:\n",
        "- Large models benefiting from parallelism\n",
        "- High throughput (batch many requests)\n",
        "- Low latency where GPU speed critical\n",
        "\n",
        "**CPUs win for**:\n",
        "- Small models (GPU overhead not worth it)\n",
        "- Low query volume (GPU sits idle)\n",
        "- Cost sensitivity at low utilization\n",
        "- Easier deployment/flexibility\n",
        "\n",
        "**Solution**: Test both. Calculate cost per request, not just raw speed. Consider utilizationâ€”idle GPU wastes money. Use GPUs for batch workloads, CPUs for sporadic queries.\n",
        "\n",
        "---\n",
        "\n",
        "**41. Latency SLAs vs answer quality tradeoffs?**\n",
        "\n",
        "**Tighter latency requires**:\n",
        "- Smaller models (less capable)\n",
        "- Shorter contexts (less information)\n",
        "- Fewer retrieval candidates (lower recall)\n",
        "- Less generation time (shorter answers)\n",
        "- Aggressive caching (staleness risk)\n",
        "\n",
        "**Looser latency allows**:\n",
        "- Better models\n",
        "- Comprehensive retrieval\n",
        "- Iterative refinement\n",
        "- Multi-stage processing\n",
        "\n",
        "**Solution**: Define acceptable latency per use case. Different queries justify different SLAs (complex queries â†’ slower OK). Use tiered service (fast path for simple, slow path for complex). Optimize within constraints, not blindly."
      ],
      "metadata": {
        "id": "cahyUc5JjPjq"
      }
    }
  ]
}